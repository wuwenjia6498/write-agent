"""
å·¥ä½œæµå¼•æ“
å®ç°9æ­¥SOPçš„AIé€»è¾‘ï¼Œæ”¯æŒæ•°æ®åº“æŒä¹…åŒ–å’Œå‘é‡æ£€ç´¢
"""

from typing import Dict, Any, Optional, List
import json
from pathlib import Path
from .ai_service import ai_service
from .db_service import db_service
from .material_processor import process_materials, classify_materials
from .search_service import search_service

class WorkflowEngine:
    """å·¥ä½œæµæ‰§è¡Œå¼•æ“"""
    
    def __init__(self):
        """åˆå§‹åŒ–å·¥ä½œæµå¼•æ“"""
        self.configs_dir = Path(__file__).parent.parent / "configs"
    
    def load_channel_config(self, channel_id: str) -> Dict[str, Any]:
        """åŠ è½½é¢‘é“é…ç½®"""
        config_file = self.configs_dir / "channels" / f"{channel_id}.json"
        with open(config_file, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def load_blocked_words(self) -> Dict[str, Any]:
        """
        åŠ è½½å±è”½è¯åº“
        æ”¯æŒ Markdown è¡¨æ ¼æ ¼å¼å’Œ JSON æ ¼å¼
        """
        # ä¼˜å…ˆä½¿ç”¨ Markdown æ ¼å¼
        md_file = self.configs_dir / "global" / "blocked_words.md"
        json_file = self.configs_dir / "global" / "blocked_words.json"
        
        if md_file.exists():
            return self._parse_blocked_words_markdown(md_file)
        elif json_file.exists():
            with open(json_file, "r", encoding="utf-8") as f:
                return json.load(f)
        else:
            return {"categories": {}}
    
    def _parse_blocked_words_markdown(self, filepath: Path) -> Dict[str, Any]:
        """
        è§£æ Markdown æ ¼å¼çš„å±è”½è¯åº“
        ä»è¡¨æ ¼ä¸­æå–ï¼šç¦ç”¨çŸ­è¯­ã€åŸå› ã€æ›¿æ¢å»ºè®®
        """
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
        
        result = {
            "blocked_words_config": {
                "version": "2.0",
                "description": "Markdown æ ¼å¼å±è”½è¯åº“",
                "format": "markdown"
            },
            "categories": {}
        }
        
        # æŒ‰äºŒçº§æ ‡é¢˜åˆ†å‰²
        import re
        sections = re.split(r'\n## ', content)
        
        for section in sections[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªï¼ˆæ ‡é¢˜å‰çš„å†…å®¹ï¼‰
            lines = section.strip().split('\n')
            if not lines:
                continue
            
            category_name = lines[0].strip()
            
            # è·³è¿‡"å®¡æ ¡æ£€æŸ¥æ¸…å•"ç­‰éè¡¨æ ¼åŒºåŸŸ
            if 'æ£€æŸ¥æ¸…å•' in category_name:
                continue
            
            patterns = []
            in_table = False
            
            for line in lines[1:]:
                line = line.strip()
                
                # è·³è¿‡è¡¨å¤´å’Œåˆ†éš”çº¿
                if line.startswith('| ç¦ç”¨çŸ­è¯­') or line.startswith('|---'):
                    in_table = True
                    continue
                
                # è§£æè¡¨æ ¼è¡Œ
                if in_table and line.startswith('|') and line.endswith('|'):
                    cells = [cell.strip() for cell in line.split('|')[1:-1]]
                    if len(cells) >= 3:
                        patterns.append({
                            "phrase": cells[0],
                            "reason": cells[1],
                            "replacement": cells[2]
                        })
            
            if patterns:
                # ç”Ÿæˆ category key
                category_key = category_name.replace('ï¼ˆ', '_').replace('ï¼‰', '').replace(' ', '_')
                result["categories"][category_key] = {
                    "name": category_name,
                    "patterns": patterns
                }
        
        return result
    
    async def execute_step_1(self, brief: str, channel_id: str) -> Dict[str, Any]:
        """
        Step 1: ç†è§£éœ€æ±‚ & ä¿å­˜Brief
        """
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªéœ€æ±‚åˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æç”¨æˆ·çš„åˆ›ä½œéœ€æ±‚ï¼Œæå–å…³é”®ä¿¡æ¯ã€‚

è¾“å‡ºæ ¼å¼ï¼š
1. ä¸»é¢˜ï¼šxxx
2. ç›®æ ‡è¯»è€…ï¼šxxx
3. æœŸæœ›å­—æ•°ï¼šxxx
4. ç‰¹æ®Šè¦æ±‚ï¼šxxx
5. å…³é”®è¯ï¼šxxx
"""
        
        think_aloud = f"ğŸ“‹ æ­£åœ¨åˆ†æéœ€æ±‚...\nç”¨æˆ·è¾“å…¥ï¼š{brief[:100]}..."
        
        result = await ai_service.generate_content(
            system_prompt=system_prompt,
            user_message=f"è¯·åˆ†æä»¥ä¸‹åˆ›ä½œéœ€æ±‚ï¼š\n\n{brief}",
            temperature=0.3
        )
        
        return {
            "output": result,
            "think_aloud": think_aloud
        }
    
    async def execute_step_2(self, brief_analysis: str, channel_id: str) -> Dict[str, Any]:
        """
        Step 2: ä¿¡æ¯æœç´¢ä¸çŸ¥è¯†ç®¡ç†
        
        æµç¨‹ï¼š
        1. ä½¿ç”¨ Tavily API è¿›è¡ŒçœŸå®ç½‘ç»œæœç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        2. å°†æœç´¢ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ä¼ ç»™ AI
        3. AI åŸºäºçœŸå®æœç´¢ç»“æœç”Ÿæˆè°ƒç ”æŠ¥å‘Š
        4. æç‚¼æ ¸å¿ƒè¦ç‚¹æ‘˜è¦
        """
        think_aloud = "ğŸ” æ­£åœ¨è¿›è¡Œæ·±åº¦è°ƒç ”...\n"
        
        # ä» brief_analysis ä¸­æå–ä¸»é¢˜å…³é”®è¯
        topic_keywords = self._extract_topic_from_brief(brief_analysis)
        think_aloud += f"  - è¯†åˆ«è°ƒç ”ä¸»é¢˜: {topic_keywords}\n"
        
        # ====================================================================
        # é˜¶æ®µé›¶ï¼šçœŸå®ç½‘ç»œæœç´¢ï¼ˆå¦‚æœ Tavily API å¯ç”¨ï¼‰
        # ====================================================================
        search_context = ""
        knowledge_sources = []
        
        if search_service.is_available():
            think_aloud += "  - ğŸŒ æ­£åœ¨è¿›è¡Œç½‘ç»œæœç´¢...\n"
            
            search_result = await search_service.search_for_research(
                topic=topic_keywords,
                context=brief_analysis
            )
            
            if search_result["result_count"] > 0:
                search_context = search_result["context"]
                knowledge_sources = search_result["sources"]
                think_aloud += f"  - âœ“ æœç´¢å®Œæˆï¼Œè·å– {search_result['result_count']} æ¡çœŸå®æ¥æº\n"
            else:
                think_aloud += "  - âš  æœç´¢æ— ç»“æœï¼Œå°†ä½¿ç”¨ AI çŸ¥è¯†åº“ç”Ÿæˆ\n"
        else:
            think_aloud += "  - âš  æœç´¢æœåŠ¡æœªé…ç½®ï¼ˆTAVILY_API_KEYï¼‰ï¼Œä½¿ç”¨ AI çŸ¥è¯†åº“\n"
        
        # ====================================================================
        # é˜¶æ®µä¸€ï¼šç”Ÿæˆè¯¦å°½è°ƒç ”èµ„æ–™
        # ====================================================================
        if search_context:
            # æœ‰æœç´¢ç»“æœï¼ŒåŸºäºçœŸå®æ¥æºç”Ÿæˆ
            research_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å†…å®¹è°ƒç ”ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ã€çœŸå®æœç´¢ç»“æœã€‘ï¼Œæ•´ç†å‡ºç»“æ„åŒ–çš„è°ƒç ”æŠ¥å‘Šã€‚

## é‡è¦è¦æ±‚
1. **å¿…é¡»åŸºäºæœç´¢ç»“æœ**ï¼šåªä½¿ç”¨æœç´¢ç»“æœä¸­çš„ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ 
2. **æ ‡æ³¨æ¥æº**ï¼šåœ¨å…³é”®ä¿¡æ¯åæ ‡æ³¨æ¥æºç¼–å·ï¼Œå¦‚ [æ¥æº1]
3. **ç»“æ„æ¸…æ™°**ï¼šæŒ‰ä¸»é¢˜åˆ†ç±»æ•´ç†

## è¾“å‡ºæ ¼å¼ï¼ˆMarkdownï¼‰

### ä¸€ã€æ ¸å¿ƒæ¦‚å¿µä¸å®šä¹‰
- xxx [æ¥æºX]

### äºŒã€å…³é”®æ•°æ®ä¸äº‹å®
- xxx [æ¥æºX]

### ä¸‰ã€ä¸“å®¶è§‚ç‚¹ä¸ç†è®ºæ”¯æ’‘
- xxx [æ¥æºX]

### å››ã€æ¡ˆä¾‹ä¸å®è¯
- xxx [æ¥æºX]

### äº”ã€å¸¸è§è¯¯åŒºä¸æ³¨æ„äº‹é¡¹
- xxx

### å…­ã€å»¶ä¼¸é˜…è¯»å»ºè®®
- å‚è€ƒæ¥æºä¸­çš„ç›¸å…³é“¾æ¥

---
è¯·åŸºäºçœŸå®æœç´¢ç»“æœç”Ÿæˆè°ƒç ”æŠ¥å‘Šï¼Œç¡®ä¿ä¿¡æ¯å¯æº¯æºã€‚"""
            
            user_message = f"""ã€åˆ›ä½œéœ€æ±‚ã€‘
{brief_analysis}

ã€çœŸå®æœç´¢ç»“æœã€‘
{search_context}

è¯·åŸºäºä»¥ä¸Šæœç´¢ç»“æœï¼Œç”Ÿæˆç»“æ„åŒ–çš„è°ƒç ”æŠ¥å‘Šã€‚"""
        else:
            # æ— æœç´¢ç»“æœï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
            research_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å†…å®¹è°ƒç ”ä¸“å®¶ã€‚è¯·æ ¹æ®åˆ›ä½œéœ€æ±‚ï¼Œè¿›è¡Œå…¨é¢æ·±å…¥çš„èµ„æ–™è°ƒç ”ã€‚

## è°ƒç ”è¦æ±‚
1. **ä¿¡æ¯å…¨é¢**ï¼šè¦†ç›–ä¸»é¢˜çš„å„ä¸ªå…³é”®ç»´åº¦
2. **æœ‰æ®å¯æŸ¥**ï¼šæ ‡æ³¨ä¿¡æ¯æ¥æºç±»å‹ï¼ˆå­¦æœ¯ç ”ç©¶/å®˜æ–¹æ•°æ®/ä¸“å®¶è§‚ç‚¹/æ¡ˆä¾‹å®è¯ï¼‰
3. **å®ç”¨å¯¼å‘**ï¼šèšç„¦å¯¹åˆ›ä½œæœ‰å®é™…ä»·å€¼çš„ä¿¡æ¯
4. **ç»“æ„æ¸…æ™°**ï¼šåˆ†ç±»æ•´ç†ï¼Œä¾¿äºåç»­å¼•ç”¨

## è¾“å‡ºæ ¼å¼ï¼ˆMarkdownï¼‰

### ä¸€ã€æ ¸å¿ƒæ¦‚å¿µä¸å®šä¹‰
- xxx

### äºŒã€å…³é”®æ•°æ®ä¸äº‹å®
- xxxï¼ˆæ¥æºï¼šxxxï¼‰

### ä¸‰ã€ä¸“å®¶è§‚ç‚¹ä¸ç†è®ºæ”¯æ’‘
- xxx

### å››ã€æ¡ˆä¾‹ä¸å®è¯
- xxx

### äº”ã€å¸¸è§è¯¯åŒºä¸æ³¨æ„äº‹é¡¹
- xxx

### å…­ã€å»¶ä¼¸é˜…è¯»å»ºè®®
- xxx

---
è¯·ç¡®ä¿å†…å®¹è¯¦å®ã€æœ‰æ·±åº¦ï¼Œä¸ºåç»­åˆ›ä½œæä¾›å……è¶³çš„ç´ ææ”¯æ’‘ã€‚"""
            
            user_message = f"è¯·æ ¹æ®ä»¥ä¸‹éœ€æ±‚åˆ†æè¿›è¡Œæ·±åº¦è°ƒç ”ï¼š\n\n{brief_analysis}"
        
        think_aloud += "  - æ­£åœ¨ç”Ÿæˆè¯¦å°½è°ƒç ”èµ„æ–™...\n"
        
        knowledge_base = await ai_service.generate_content(
            system_prompt=research_prompt,
            user_message=user_message,
            temperature=0.4,
            max_tokens=4000
        )
        
        think_aloud += f"  - âœ“ è°ƒç ”èµ„æ–™ç”Ÿæˆå®Œæˆ ({len(knowledge_base)} å­—)\n"
        
        # ====================================================================
        # é˜¶æ®µäºŒï¼šæç‚¼æ ¸å¿ƒè¦ç‚¹æ‘˜è¦ï¼ˆ300å­—ä»¥å†…ï¼‰
        # ====================================================================
        summary_prompt = """ä½ æ˜¯ä¸€ä½æ“…é•¿ä¿¡æ¯æç‚¼çš„ç¼–è¾‘ã€‚è¯·å°†è°ƒç ”èµ„æ–™æç‚¼ä¸ºç®€æ´çš„æ ¸å¿ƒè¦ç‚¹ã€‚

ã€ä¸¥æ ¼è¦æ±‚ã€‘
- æ€»å­—æ•°ï¼š200-300å­—
- è¦ç‚¹æ•°ï¼š3-5ä¸ªæ ¸å¿ƒå‘ç°
- æ ¼å¼ï¼šçº¯æ–‡æœ¬ï¼Œç¦æ­¢ä½¿ç”¨ä»»ä½• Markdown æˆ– HTML æ ‡ç­¾

ã€è¾“å‡ºæ ¼å¼ç¤ºä¾‹ã€‘
æ ¸å¿ƒå‘ç°ï¼š
1. ç¬¬ä¸€ä¸ªè¦ç‚¹çš„ä¸€å¥è¯æ¦‚æ‹¬
2. ç¬¬äºŒä¸ªè¦ç‚¹çš„ä¸€å¥è¯æ¦‚æ‹¬
3. ç¬¬ä¸‰ä¸ªè¦ç‚¹çš„ä¸€å¥è¯æ¦‚æ‹¬

åˆ›ä½œå»ºè®®ï¼šä¸€å¥è¯è¯´æ˜è¿™äº›å‘ç°å¯¹æ–‡ç« åˆ›ä½œçš„æŒ‡å¯¼æ„ä¹‰ã€‚

ã€ç¦æ­¢äº‹é¡¹ã€‘
- ä¸è¦ä½¿ç”¨ # ## ### ç­‰æ ‡é¢˜ç¬¦å·
- ä¸è¦ä½¿ç”¨ ** __ ç­‰åŠ ç²—ç¬¦å·
- ä¸è¦ä½¿ç”¨ <strong> <b> ç­‰ HTML æ ‡ç­¾
- ç›´æ¥è¾“å‡ºçº¯æ–‡æœ¬å³å¯"""
        
        think_aloud += "  - æ­£åœ¨æç‚¼æ ¸å¿ƒè¦ç‚¹æ‘˜è¦...\n"
        
        knowledge_summary = await ai_service.generate_content(
            system_prompt=summary_prompt,
            user_message=f"è¯·å°†ä»¥ä¸‹è°ƒç ”èµ„æ–™æç‚¼ä¸º 300 å­—ä»¥å†…çš„æ ¸å¿ƒè¦ç‚¹æ‘˜è¦ï¼š\n\n{knowledge_base}",
            temperature=0.3,
            max_tokens=500
        )
        
        think_aloud += "  - âœ“ æ‘˜è¦æç‚¼å®Œæˆ\n"
        think_aloud += "\nğŸ“š è°ƒç ”é˜¶æ®µå®Œæˆï¼Œè¯·å®¡é˜…å¹¶ç¡®è®¤è°ƒç ”ç»“è®ºã€‚"
        
        return {
            "output": knowledge_base,           # å®Œæ•´è°ƒç ”èµ„æ–™
            "knowledge_summary": knowledge_summary,  # æ ¸å¿ƒè¦ç‚¹æ‘˜è¦
            "knowledge_sources": knowledge_sources,  # çœŸå®æœç´¢æ¥æº
            "think_aloud": think_aloud,
            "is_checkpoint": True  # è®¾ä¸ºå¡ç‚¹ï¼Œéœ€ç”¨æˆ·ç¡®è®¤
        }
    
    async def execute_step_3(self, brief_analysis: str, channel_id: str) -> Dict[str, Any]:
        """
        Step 3: é€‰é¢˜è®¨è®ºï¼ˆå¿…åšå¡ç‚¹ï¼‰
        """
        channel_config = self.load_channel_config(channel_id)
        
        system_prompt = f"""{channel_config['system_prompt']['role']}

è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚æä¾›3-4ä¸ªé€‰é¢˜æ–¹å‘ã€‚

æ¯ä¸ªé€‰é¢˜åŒ…å«ï¼š
1. æ ‡é¢˜ï¼ˆå¸å¼•äººä½†ä¸æ ‡é¢˜å…šï¼‰
2. æ ¸å¿ƒè§‚ç‚¹
3. å¤§çº²ï¼ˆ3-5ä¸ªè¦ç‚¹ï¼‰
4. é¢„ä¼°å·¥ä½œé‡ï¼ˆå­—æ•°ã€æ‰€éœ€ç´ æï¼‰
5. ä¼˜åŠ£åˆ†æ

## âš ï¸ ç¦ç”¨ä¹¦ç›®ï¼ˆé¿å…AIå‘³ï¼‰
ä¸¾ä¾‹æ—¶ç¦æ­¢ä½¿ç”¨ä»¥ä¸‹è¢«è¿‡åº¦å¼•ç”¨çš„å¸¸è§ä¹¦ç›®ï¼š
ã€Šå¤æ´›çš„ç½‘ã€‹ã€Šå°ç‹å­ã€‹ã€Šçª—è¾¹çš„å°è±†è±†ã€‹ã€Šçˆ±å¿ƒæ ‘ã€‹ã€ŠçŒœçŒœæˆ‘æœ‰å¤šçˆ±ä½ ã€‹ã€Šé€ƒå®¶å°å…”ã€‹ã€Šå¥½é¥¿çš„æ¯›æ¯›è™«ã€‹ã€Šå¤§å«ä¸å¯ä»¥ã€‹
- å¦‚éœ€ä¹¦ç±ä¸¾ä¾‹ï¼Œè¯·é€‰æ‹©æ›´å°ä¼—ä½†åŒæ ·ä¼˜è´¨çš„ä½œå“

å†™ä½œé£æ ¼è¦æ±‚ï¼š
{chr(10).join(['- ' + style for style in channel_config['system_prompt']['writing_style']])}
"""
        
        think_aloud = f"ğŸ’¡ æ­£åœ¨ä¸º'{channel_config['channel_name']}'é¢‘é“ç”Ÿæˆé€‰é¢˜æ–¹æ¡ˆ...\n\næ€è€ƒé‡ç‚¹ï¼š\n- ç¬¦åˆé¢‘é“è°ƒæ€§\n- é¿å…ç©ºæ´å¥—è¯\n- æœ‰ç‹¬ç‰¹è§†è§’"
        
        result = await ai_service.generate_content(
            system_prompt=system_prompt,
            user_message=f"åŸºäºä»¥ä¸‹éœ€æ±‚åˆ†æï¼Œæä¾›3-4ä¸ªé€‰é¢˜æ–¹å‘ï¼š\n\n{brief_analysis}",
            temperature=0.8,
            max_tokens=6000
        )
        
        return {
            "output": result,
            "think_aloud": think_aloud,
            "is_checkpoint": True  # å¡ç‚¹ï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤
        }
    
    async def execute_step_4(self, selected_topic: str) -> Dict[str, Any]:
        """
        Step 4: åˆ›å»ºåä½œæ–‡æ¡£
        """
        system_prompt = """ä½ æ˜¯é¡¹ç›®ç®¡ç†ä¸“å®¶ã€‚æ ¹æ®é€‰å®šçš„é€‰é¢˜ï¼Œåˆ›å»ºåä½œæ¸…å•ã€‚

è¾“å‡ºæ ¼å¼ï¼š
## AIè´Ÿè´£çš„ä»»åŠ¡
- [ ] ä»»åŠ¡1
- [ ] ä»»åŠ¡2

## ç”¨æˆ·éœ€è¦æä¾›çš„å†…å®¹
- [ ] çœŸå®æ¡ˆä¾‹ï¼šxxx
- [ ] ä¸ªäººè§‚ç‚¹ï¼šxxx
- [ ] æ•°æ®æ”¯æŒï¼šxxx

## æ³¨æ„äº‹é¡¹
- ä¸ç¼–é€ æ•°æ®
- ä¸ä½¿ç”¨å¥—è¯
"""
        
        think_aloud = "ğŸ“ æ­£åœ¨ç”Ÿæˆåä½œæ¸…å•..."
        
        result = await ai_service.generate_content(
            system_prompt=system_prompt,
            user_message=f"ä¸ºä»¥ä¸‹é€‰é¢˜åˆ›å»ºåä½œæ¸…å•ï¼š\n\n{selected_topic}",
            temperature=0.3
        )
        
        return {
            "output": result,
            "think_aloud": think_aloud
        }
    
    async def execute_step_5(
        self, 
        selected_topic: str, 
        channel_id: str,
        task_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Step 5: é£æ ¼å»ºæ¨¡ä¸ç´ ææ£€ç´¢ (v3.5 - æ ·æ–‡çŸ©é˜µæ¨¡å¼)
        
        æ ¸å¿ƒå˜åŒ–ï¼š
        1. åˆ‡æ¢ä¸º"æ ·æ–‡çŸ©é˜µ"æ¨¡å¼ï¼Œæ¯ç¯‡æ ·æ–‡ç‹¬ç«‹ä¿æŒ 6 ç»´ç‰¹å¾
        2. æ™ºèƒ½æ¨èæœ€åŒ¹é…çš„å•ç¯‡æ ·æ–‡ï¼ˆåŸºäº custom_tags + Brief å…³é”®è¯ï¼‰
        3. ä¸»ç¼–å¯åœ¨å‰ç«¯é€‰æ‹©/åˆ‡æ¢å‚è€ƒæ ·æ–‡
        4. Step 7 å°†ä½¿ç”¨æ‰€é€‰æ ·æ–‡çš„ç‹¬ç«‹ style_profile
        """
        channel_config = self.load_channel_config(channel_id)
        
        think_aloud = "[Step 5] å¼€å§‹é£æ ¼å»ºæ¨¡ä¸ç´ ææ£€ç´¢ (æ ·æ–‡çŸ©é˜µæ¨¡å¼)...\n"
        
        # è·å–é¢‘é“æ•°æ®
        channel_data = db_service.get_channel_by_slug(channel_id)
        
        # ====================================================================
        # 1. æ ·æ–‡çŸ©é˜µï¼šè·å–æ‰€æœ‰å·²åˆ†æçš„æ ·æ–‡ï¼Œæ™ºèƒ½æ¨èæœ€åŒ¹é…çš„ä¸€ç¯‡
        # ====================================================================
        think_aloud += "\n[æ ·æ–‡çŸ©é˜µ] æ­£åœ¨åŠ è½½æ ·æ–‡åº“...\n"
        
        style_profile = None
        recommended_sample = None
        all_samples = []
        
        # v3.5: ä»ç‹¬ç«‹è¡¨è·å–æ ·æ–‡
        if channel_data:
            # æå–å…³é”®è¯ç”¨äºåŒ¹é…
            keywords = self._extract_keywords(selected_topic)
            think_aloud += f"  - é€‰é¢˜å…³é”®è¯: {', '.join(keywords[:5])}\n"
            
            # è·å–æ ·æ–‡å¹¶è®¡ç®—åŒ¹é…åˆ†æ•°
            all_samples = db_service.get_style_samples_for_matching(
                channel_id=channel_data['id'],
                keywords=keywords
            )
            
            if all_samples:
                think_aloud += f"  - âœ“ æ‰¾åˆ° {len(all_samples)} ç¯‡å·²åˆ†æçš„æ ·æ–‡\n"
                
                # æ¨èåŒ¹é…åº¦æœ€é«˜çš„æ ·æ–‡
                recommended_sample = all_samples[0]
                matched_tags = recommended_sample.get('matched_tags', [])
                
                think_aloud += f"\n[æ™ºèƒ½æ¨è] æœ€ä½³åŒ¹é…æ ·æ–‡ï¼šã€Š{recommended_sample['title']}ã€‹\n"
                if matched_tags:
                    think_aloud += f"  - åŒ¹é…æ ‡ç­¾: {', '.join(matched_tags)}\n"
                think_aloud += f"  - åŒ¹é…åˆ†æ•°: {recommended_sample.get('match_score', 0)}\n"
                
                # ä½¿ç”¨æ¨èæ ·æ–‡çš„ style_profile ä½œä¸ºé»˜è®¤
                style_profile = recommended_sample.get('style_profile', {})
                
                # å±•ç¤ºè¯¥æ ·æ–‡çš„ 6 ç»´ç‰¹å¾æ‘˜è¦
                if style_profile:
                    dims = style_profile
                    think_aloud += "  - 6 ç»´ç‰¹å¾:\n"
                    if dims.get('opening_style'):
                        think_aloud += f"    Â· å¼€å¤´: {dims['opening_style'].get('type', '-')}\n"
                    if dims.get('tone'):
                        think_aloud += f"    Â· è¯­æ°”: {dims['tone'].get('type', '-')}\n"
                    if dims.get('ending_style'):
                        think_aloud += f"    Â· ç»“å°¾: {dims['ending_style'].get('type', '-')}\n"
            else:
                think_aloud += "  - âš  è¯¥é¢‘é“æš‚æ— å·²åˆ†æçš„æ ·æ–‡\n"
        
        # å›é€€ï¼šå¦‚æœæ²¡æœ‰ç‹¬ç«‹è¡¨æ ·æ–‡ï¼Œå°è¯•ä»æ—§ JSONB å­—æ®µè·å–
        if not style_profile:
            style_samples = channel_data.get('style_samples', []) if channel_data else []
            
            if channel_data and channel_data.get('style_profile'):
                style_profile = channel_data['style_profile']
                think_aloud += f"  - å›é€€åˆ°é¢‘é“æ•´ä½“é£æ ¼ç”»åƒ\n"
            elif style_samples:
                # ä½¿ç”¨ç¬¬ä¸€ç¯‡æ ·æ–‡çš„ç‰¹å¾
                for sample in style_samples:
                    if sample.get('features'):
                        style_profile = sample['features']
                        think_aloud += f"  - ä½¿ç”¨æ ·æ–‡ã€Š{sample.get('title', 'æ— æ ‡é¢˜')}ã€‹çš„ç‰¹å¾\n"
                        break
        
        # æœ€ç»ˆå›é€€ï¼šé»˜è®¤é£æ ¼
        if not style_profile:
            think_aloud += "  - âš  ä½¿ç”¨é»˜è®¤é£æ ¼é…ç½®\n"
            style_profile = {
                "style_portrait": "ä¸“ä¸šè€Œäº²åˆ‡çš„å†…å®¹åˆ›ä½œè€…ï¼Œç”¨çœŸè¯šçš„æ€åº¦åˆ†äº«è§‚ç‚¹",
                "structural_logic": ["åœºæ™¯åˆ‡å…¥", "é—®é¢˜å¼•å‡º", "è§‚ç‚¹å±•å¼€", "æ¡ˆä¾‹æ”¯æ’‘", "æ€»ç»“å‡å"],
                "tone_features": ["çœŸè¯š", "ä¸“ä¸š", "äº²åˆ‡"],
                "opening_style": {"type": "story_intro", "description": "å»ºè®®ç”¨ç”Ÿæ´»åœºæ™¯å¼•å…¥"},
                "tone": {"type": "warm_friend", "formality": 0.3, "description": "æ¸©æ¶¦äº²åˆ‡ï¼Œåƒæœ‹å‹èŠå¤©"},
                "ending_style": {"type": "reflection", "description": "å¼•å¯¼è¯»è€…æ€è€ƒ"},
                "writing_guidelines": ["é¿å…è¯´æ•™è¯­æ°”", "å¤šç”¨çŸ­å¥", "èå…¥çœŸå®ç»å†"]
            }
        
        # ====================================================================
        # 2. ä»æ•°æ®åº“æ£€ç´¢çœŸå®ç´ æ
        # ====================================================================
        think_aloud += "\n[RAG] æ­£åœ¨ä»ç´ æåº“æ£€ç´¢ç›¸å…³ç´ æ...\n"
        think_aloud += f"  - é¢‘é“è¿‡æ»¤: {channel_id}\n"
        
        retrieved_materials = []
        raw_materials = []
        
        if channel_data:
            keywords = self._extract_keywords(selected_topic)
            think_aloud += f"  - æ£€ç´¢å…³é”®è¯: {', '.join(keywords)}\n"
            
            # å¢å¤§æ£€ç´¢é‡ï¼Œä¸ºå»é‡ç•™ä½™é‡
            raw_materials = db_service.search_materials_by_keywords(
                channel_id=channel_data["id"],
                keywords=keywords,
                limit=15
            )
            think_aloud += f"  - åŸå§‹æ£€ç´¢: {len(raw_materials)} æ¡ç´ æ\n"
        
        if not raw_materials and channel_data:
            think_aloud += "  - æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œè·å–é¢‘é“é€šç”¨ç´ æ...\n"
            raw_materials = db_service.get_materials_by_channel(
                channel_id=channel_data["id"],
                limit=15
            )
        
        # ====================================================================
        # 2.1 ç´ ææ¸…æ´—ä¸å»é‡
        # ====================================================================
        if raw_materials:
            think_aloud += "\n[ç´ æå¤„ç†] å¼€å§‹æ¸…æ´—ä¸å»é‡...\n"
            original_count = len(raw_materials)
            
            # è°ƒç”¨ç´ æå¤„ç†å™¨ï¼šå™ªå£°è¿‡æ»¤ + æ¥æºå»é‡ + å†…å®¹å»é‡
            retrieved_materials = process_materials(
                raw_materials,
                enable_spam_filter=True,
                enable_source_dedupe=True,
                enable_content_dedupe=True,
                content_similarity_threshold=0.85
            )
            
            # é™åˆ¶æœ€ç»ˆæ•°é‡
            retrieved_materials = retrieved_materials[:8]
            
            removed_count = original_count - len(retrieved_materials)
            if removed_count > 0:
                think_aloud += f"  - æ¸…æ´—å®Œæˆ: {original_count} -> {len(retrieved_materials)} æ¡\n"
                think_aloud += f"  - ç§»é™¤ {removed_count} æ¡é‡å¤/è¥é”€å†…å®¹\n"
            else:
                think_aloud += f"  - æ¸…æ´—å®Œæˆ: {len(retrieved_materials)} æ¡æœ‰æ•ˆç´ æ\n"
        
        # ====================================================================
        # 2.2 ç´ æåˆ†ç±»ï¼ˆé•¿æ–‡ vs çŸ­ç¢ï¼‰
        # ====================================================================
        classified_materials = {"long": [], "short": []}
        if retrieved_materials:
            classified_materials = classify_materials(retrieved_materials, long_threshold=200)
            think_aloud += f"  - åˆ†ç±»: {len(classified_materials['long'])} æ¡é•¿æ–‡ + {len(classified_materials['short'])} æ¡çµæ„Ÿç¢ç‰‡\n"
        
        # ====================================================================
        # 3. æ ¼å¼åŒ–è¾“å‡º
        # ====================================================================
        materials_context = ""
        if retrieved_materials:
            materials_context = "\n\n## ä»ç´ æåº“æ£€ç´¢åˆ°çš„çœŸå®ç´ æ\n"
            materials_context += "ï¼ˆä»¥ä¸‹ç´ ææ¥è‡ª15å¹´ç§¯ç´¯çš„çœŸå®ç»å†ï¼Œè¯·åœ¨åˆ›ä½œä¸­è‡ªç„¶èå…¥ï¼‰\n\n"
            
            for i, mat in enumerate(retrieved_materials, 1):
                materials_context += f"### ç´ æ{i} [{mat['material_type']}]\n"
                materials_context += f"{mat['content']}\n"
                if mat.get('source'):
                    materials_context += f"*æ¥æº: {mat['source']}*\n"
                materials_context += "\n"
            
            think_aloud += f"\n[RAG] å·²æ³¨å…¥ {len(retrieved_materials)} æ¡çœŸå®ç´ æ\n"
        else:
            think_aloud += "\n[WARN] ç´ æåº“ä¸­æš‚æ— ç›¸å…³ç´ æï¼Œè¯·åœ¨åˆ›ä½œæ—¶æ³¨å…¥çœŸå®ç»å†\n"
        
        # ====================================================================
        # 4. ç”Ÿæˆé£æ ¼æŒ‡å¯¼æ–‡æ¡£ (v3.5 ç®€åŒ–ï¼Œä¸å†æ˜¾ç¤º JSON)
        # ====================================================================
        
        # æ„å»ºé£æ ¼æ‘˜è¦ï¼ˆå» JSON åŒ–ï¼‰
        style_summary = ""
        if style_profile:
            if style_profile.get('style_portrait'):
                style_summary += f"**é£æ ¼ç”»åƒ**ï¼š{style_profile['style_portrait']}\n\n"
            
            if style_profile.get('structural_logic'):
                logic = style_profile['structural_logic']
                style_summary += f"**ç»“æ„é€»è¾‘**ï¼š{' â†’ '.join(logic[:5])}\n\n"
            
            if style_profile.get('tone_features'):
                style_summary += f"**è¯­æ°”ç‰¹å¾**ï¼š{', '.join(style_profile['tone_features'][:4])}\n\n"
            
            if style_profile.get('writing_guidelines'):
                guidelines = style_profile['writing_guidelines']
                style_summary += "**åˆ›ä½œæŒ‡å—**ï¼š\n"
                for i, g in enumerate(guidelines[:5], 1):
                    style_summary += f"  {i}. {g}\n"
        
        sample_info = ""
        if recommended_sample:
            sample_info = f"""
## æ¨èå‚è€ƒæ ·æ–‡
- **æ ‡é¢˜**ï¼šã€Š{recommended_sample['title']}ã€‹
- **æ ‡ç­¾**ï¼š{', '.join(recommended_sample.get('custom_tags', []) or ['æ— æ ‡ç­¾'])}
- **åŒ¹é…åº¦**ï¼š{recommended_sample.get('match_score', 0)} åˆ†

> æœ¬æ¬¡åˆ›ä½œå°†å‚è€ƒæ­¤æ ·æ–‡çš„å†™ä½œèŒƒå¼ã€‚å¦‚éœ€æ›´æ¢ï¼Œè¯·åœ¨å·¥ä½œå°é€‰æ‹©å…¶ä»–æ ·æ–‡ã€‚
"""
        
        style_guide = f"""## æœ¬ç¯‡åˆ›ä½œé£æ ¼æŒ‡å¼•

{style_summary}
{sample_info}

## åˆ›ä½œè¦æ±‚
1. **ä¸¥æ ¼æ¨¡ä»¿æ‰€é€‰æ ·æ–‡çš„å†™ä½œèŒƒå¼**ï¼ˆå¼€å¤´æ–¹å¼ã€å¥å¼èŠ‚å¥ã€è¯­æ°”ç‰¹ç‚¹ã€ç»“å°¾é£æ ¼ï¼‰
2. **çœŸå®ç´ æä¼˜å…ˆ**ï¼šå°†æ£€ç´¢åˆ°çš„çœŸå®ç»å†è‡ªç„¶èå…¥ï¼Œç¦æ­¢å‡­ç©ºç¼–é€ æ¡ˆä¾‹
3. **ä¿æŒé¢‘é“è°ƒæ€§**ï¼š{channel_config.get('brand_personality', 'æ¸©æ¶¦ã€ä¸“ä¸šã€æœ‰æ·±åº¦')}

{materials_context}
"""
        
        # æŒä¹…åŒ– Think Aloud
        if task_id:
            db_service.add_think_aloud_log(task_id, 5, think_aloud)
        
        return {
            "output": style_guide,
            "think_aloud": think_aloud,
            "retrieved_materials": retrieved_materials,
            "classified_materials": classified_materials,
            "style_profile": style_profile,
            # v3.5 æ–°å¢ï¼šæ ·æ–‡æ¨èæ•°æ®
            "recommended_sample": recommended_sample,
            "all_samples": all_samples,
            "has_sample_recommendation": bool(recommended_sample)
        }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰"""
        # ç§»é™¤å¸¸è§åœç”¨è¯ï¼Œæå–å…³é”®è¯
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'å’Œ', 'äº†', 'ä¸', 'å¯¹', 'ä¸º', 'ä»¥', 'ç­‰', 
                      'è¿™', 'é‚£', 'å°±', 'ä¹Ÿ', 'éƒ½', 'è¦', 'èƒ½', 'ä¼š', 'å¯ä»¥', 'åº”è¯¥',
                      'ä¸€ä¸ª', 'æˆ‘ä»¬', 'ä»–ä»¬', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ'}
        
        # ç®€å•åˆ†è¯ï¼ˆæŒ‰æ ‡ç‚¹å’Œç©ºæ ¼ï¼‰
        import re
        words = re.split(r'[ï¼Œã€‚ã€ï¼ï¼Ÿï¼šï¼›""''ï¼ˆï¼‰\s]+', text)
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        keywords = [
            w.strip() for w in words 
            if w.strip() and len(w.strip()) >= 2 and w.strip() not in stop_words
        ]
        
        # å–å‰10ä¸ªå…³é”®è¯
        return keywords[:10]
    
    def _extract_topic_from_brief(self, brief_analysis: str) -> str:
        """
        ä»éœ€æ±‚åˆ†æä¸­æå–ä¸»é¢˜å…³é”®è¯ç”¨äºæœç´¢
        
        Args:
            brief_analysis: Step 1 ç”Ÿæˆçš„éœ€æ±‚åˆ†æ
            
        Returns:
            é€‚åˆæœç´¢çš„ä¸»é¢˜å…³é”®è¯å­—ç¬¦ä¸²
        """
        import re
        
        # å°è¯•ä»ç»“æ„åŒ–åˆ†æä¸­æå–ä¸»é¢˜
        topic_match = re.search(r'ä¸»é¢˜[ï¼š:]\s*(.+?)(?:\n|$)', brief_analysis)
        if topic_match:
            return topic_match.group(1).strip()
        
        # å°è¯•æå–å…³é”®è¯
        keywords_match = re.search(r'å…³é”®è¯[ï¼š:]\s*(.+?)(?:\n|$)', brief_analysis)
        if keywords_match:
            return keywords_match.group(1).strip()
        
        # å›é€€ï¼šæå–å‰å‡ ä¸ªå…³é”®è¯
        keywords = self._extract_keywords(brief_analysis)
        if keywords:
            return ' '.join(keywords[:5])
        
        # æœ€åå›é€€ï¼šè¿”å›å‰50ä¸ªå­—ç¬¦
        return brief_analysis[:50].replace('\n', ' ')
    
    async def execute_step_6(self) -> Dict[str, Any]:
        """
        Step 6: æŒ‚èµ·ç­‰å¾…ï¼ˆæ•°æ®ç¡®è®¤å¡ç‚¹ï¼‰
        """
        think_aloud = "â¸ï¸ æŒ‚èµ·ç­‰å¾…ç”¨æˆ·ç¡®è®¤æ‰€æœ‰å¿…éœ€ç´ æå·²å°±ç»ª..."
        
        result = """## æ•°æ®ç¡®è®¤æ¸…å•

è¯·ç¡®è®¤ä»¥ä¸‹å†…å®¹å·²å‡†å¤‡å¥½ï¼š
- [ ] çœŸå®æ¡ˆä¾‹å’Œç»å†
- [ ] ä¸ªäººè§‚ç‚¹å’Œæ€åº¦
- [ ] å¿…è¦çš„æ•°æ®æ”¯æŒ
- [ ] å…¶ä»–å…³é”®ä¿¡æ¯

âš ï¸ é‡è¦æé†’ï¼š
- ç»ä¸ç¼–é€ è™šå‡ä¿¡æ¯
- å®å¯ç­‰å¾…ä¹Ÿä¸çå†™
- æ‰€æœ‰æ•°æ®å¿…é¡»æœ‰æ¥æº

ç¡®è®¤æ— è¯¯åï¼Œç‚¹å‡»"ç»§ç»­"è¿›å…¥åˆ›ä½œé˜¶æ®µã€‚
"""
        
        return {
            "output": result,
            "think_aloud": think_aloud,
            "is_checkpoint": True  # å¡ç‚¹ï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤
        }
    
    async def execute_step_7(
        self,
        selected_topic: str,
        style_guide: str,
        materials: str,
        channel_id: str,
        word_count: int = 1500,  # é»˜è®¤å­—æ•°é™åˆ¶
        style_profile: Dict[str, Any] = None,  # é£æ ¼ç”»åƒ
        selected_sample: Dict[str, Any] = None  # v3.5: æ‰€é€‰çš„å•ä¸€æ ‡æ†æ ·æ–‡
    ) -> Dict[str, Any]:
        """
        Step 7: åˆç¨¿åˆ›ä½œï¼ˆv3.5 - å•ä¸€æ ‡æ†é©±åŠ¨ï¼‰
        
        æ ¸å¿ƒå˜åŒ–ï¼š
        1. ä¼˜å…ˆä½¿ç”¨æ‰€é€‰æ ·æ–‡çš„ç‹¬ç«‹ style_profileï¼ˆå•ä¸€æ ‡æ†ï¼‰
        2. å¦‚æœç”¨æˆ·ä¿®æ”¹äº†åˆ›ä½œæŒ‡å—ï¼Œäººå·¥å¹²é¢„è¦†ç›–æ ·æ–‡é»˜è®¤ç‰¹å¾
        3. ä¸¥ç¦å‡­ç©ºç¼–é€ æ¡ˆä¾‹
        
        ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š
        1. ç”¨æˆ·ç‰¹æ®Šè¦æ±‚ (custom_requirement)
        2. ç”¨æˆ·ä¿®æ”¹çš„åˆ›ä½œæŒ‡å— (writing_guidelines)
        3. æ‰€é€‰æ ·æ–‡çš„ 6 ç»´ç‰¹å¾çº¦æŸ (selected_sample.style_profile)
        4. é¢‘é“åŸºæœ¬äººæ ¼è®¾å®š
        """
        channel_config = self.load_channel_config(channel_id)
        
        # ================================================================
        # v3.5: ç¡®å®šä½¿ç”¨çš„é£æ ¼æ¥æº
        # ================================================================
        effective_style_profile = None
        style_source = "é»˜è®¤"
        
        # ä¼˜å…ˆçº§1: ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰çš„ style_profileï¼ˆå¦‚æœæœ‰ï¼‰
        if style_profile and style_profile.get('is_customized'):
            effective_style_profile = style_profile
            style_source = "ç”¨æˆ·è‡ªå®šä¹‰"
        # ä¼˜å…ˆçº§2: ä½¿ç”¨æ‰€é€‰æ ·æ–‡çš„ç‹¬ç«‹ style_profile
        elif selected_sample and selected_sample.get('style_profile'):
            effective_style_profile = selected_sample['style_profile']
            style_source = f"æ ·æ–‡ã€Š{selected_sample.get('title', 'æœªçŸ¥')}ã€‹"
        # ä¼˜å…ˆçº§3: ä½¿ç”¨ä¼ å…¥çš„ style_profileï¼ˆå¯èƒ½æ˜¯é¢‘é“æ•´ä½“ç”»åƒï¼‰
        elif style_profile:
            effective_style_profile = style_profile
            style_source = "é¢‘é“é£æ ¼ç”»åƒ"
        
        # ================================================================
        # æå–é£æ ¼å…³é”®ä¿¡æ¯
        # ================================================================
        style_instructions = ""
        structural_logic = []
        writing_guidelines = []
        
        # è·å–ç»“æ„é€»è¾‘å’Œåˆ›ä½œæŒ‡å—
        if effective_style_profile:
            structural_logic = effective_style_profile.get('structural_logic', [])
            writing_guidelines = effective_style_profile.get('writing_guidelines', [])
            
            # ç›´æ¥ä½¿ç”¨æ ·æ–‡çš„ç‰¹å¾ï¼ˆä¸å†åŒºåˆ† stable_features å’Œ dimensionsï¼‰
            dims = effective_style_profile
        else:
            dims = {}
        
        if dims or structural_logic or writing_guidelines:
            style_instructions = """
## ğŸ¨ ã€å¼ºåˆ¶ã€‘é£æ ¼ DNA å¯¹é½ï¼ˆå¿…é¡» 100% éµå®ˆï¼‰

"""
            # ============================================================
            # 1. ç»“æ„é€»è¾‘ï¼ˆå¿…é¡»æŒ‰æ­¤é¡ºåºç»„ç»‡æ®µè½ï¼‰
            # ============================================================
            if structural_logic:
                style_instructions += "### ğŸ“ æ®µè½ç»“æ„ï¼ˆå¿…é¡»æŒ‰æ­¤é¡ºåºï¼‰\n"
                for i, step in enumerate(structural_logic[:6], 1):
                    style_instructions += f"  {i}. {step}\n"
                style_instructions += "\n**è¦æ±‚**ï¼šæ–‡ç« å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°ç»“æ„å®‰æ’æ®µè½ï¼Œä¸å¾—é—æ¼æˆ–ä¹±åºã€‚\n\n"
            
            # ============================================================
            # 2. å…­ç»´é£æ ¼ç‰¹å¾
            # ============================================================
            if dims:
                style_instructions += "### ğŸ­ å…­ç»´é£æ ¼ç‰¹å¾\n\n"
                
                # å¼€å¤´æ–¹å¼
                if dims.get('opening_style'):
                    opening = dims['opening_style']
                    style_instructions += f"**ã€å¼€å¤´ã€‘** ç±»å‹ï¼š{opening.get('type', 'æ•…äº‹å¼•å…¥')}\n"
                    if opening.get('description'):
                        style_instructions += f"  - è¦æ±‚ï¼š{opening['description']}\n"
                    if opening.get('examples'):
                        style_instructions += f"  - å‚è€ƒï¼šã€Œ{opening['examples'][0][:50]}...ã€\n"
                    style_instructions += "\n"
                
                # å¥å¼ç‰¹å¾
                if dims.get('sentence_pattern'):
                    sp = dims['sentence_pattern']
                    short_ratio = sp.get('short_ratio', 0.6)
                    style_instructions += f"**ã€å¥å¼ã€‘** çŸ­å¥å æ¯”ï¼š{short_ratio * 100:.0f}%\n"
                    if sp.get('favorite_punctuation'):
                        style_instructions += f"  - å¸¸ç”¨æ ‡ç‚¹ï¼š{', '.join(sp['favorite_punctuation'][:5])}\n"
                    if sp.get('description'):
                        style_instructions += f"  - ç‰¹å¾ï¼š{sp['description']}\n"
                    style_instructions += "\n"
                
                # æ®µè½èŠ‚å¥
                if dims.get('paragraph_rhythm'):
                    pr = dims['paragraph_rhythm']
                    style_instructions += f"**ã€æ®µè½ã€‘** èŠ‚å¥å˜åŒ–ï¼š{pr.get('variation', 'medium')}\n"
                    if pr.get('description'):
                        style_instructions += f"  - ç‰¹å¾ï¼š{pr['description']}\n"
                    style_instructions += "\n"
                
                # è¯­æ°”ç‰¹ç‚¹
                if dims.get('tone'):
                    tone = dims['tone']
                    formality = tone.get('formality', 0.3)
                    style_instructions += f"**ã€è¯­æ°”ã€‘** ç±»å‹ï¼š{tone.get('type', 'æ¸©æ¶¦äº²åˆ‡')}\n"
                    style_instructions += f"  - æ­£å¼åº¦ï¼š{formality * 100:.0f}%ï¼ˆ{'å£è¯­åŒ–' if formality < 0.4 else 'åŠæ­£å¼' if formality < 0.7 else 'æ­£å¼'}ï¼‰\n"
                    if tone.get('description'):
                        style_instructions += f"  - è¦æ±‚ï¼š{tone['description']}\n"
                    style_instructions += "\n"
                
                # ç»“å°¾é£æ ¼
                if dims.get('ending_style'):
                    ending = dims['ending_style']
                    style_instructions += f"**ã€ç»“å°¾ã€‘** ç±»å‹ï¼š{ending.get('type', 'å¼•å¯¼æ€è€ƒ')}\n"
                    if ending.get('description'):
                        style_instructions += f"  - è¦æ±‚ï¼š{ending['description']}\n"
                    style_instructions += "\n"
                
                # å¸¸ç”¨è¡¨è¾¾
                if dims.get('expressions'):
                    expr = dims['expressions']
                    if expr.get('high_freq_words'):
                        style_instructions += f"**ã€æ¨èç”¨è¯ã€‘** {', '.join(expr['high_freq_words'][:8])}\n"
                    if expr.get('avoid_words'):
                        style_instructions += f"**ã€ç¦æ­¢ç”¨è¯ã€‘** {', '.join(expr['avoid_words'][:8])}ï¼ˆä¸€æ—¦å‡ºç°å³ä¸ºä¸åˆæ ¼ï¼‰\n"
                    style_instructions += "\n"
            
            # ============================================================
            # 3. åˆ›ä½œæŒ‡å—ï¼ˆæ¯æ¡éƒ½æ˜¯ç¡¬æ€§è¦æ±‚ï¼‰
            # ============================================================
            if writing_guidelines:
                style_instructions += "### ğŸ“‹ åˆ›ä½œæŒ‡å—ï¼ˆæ¯æ¡éƒ½æ˜¯ç¡¬æ€§è§„åˆ™ï¼‰\n"
                for i, guideline in enumerate(writing_guidelines[:10], 1):
                    style_instructions += f"  {i}. âœ… {guideline}\n"
                style_instructions += "\n**å®¡æ ¡æ ‡å‡†**ï¼šä¸Šè¿°æ¯æ¡æŒ‡å—éƒ½å°†åœ¨ Step 8 å®¡æ ¡ä¸­é€ä¸€æ£€æŸ¥ï¼Œä¸ç¬¦åˆçš„å°†è¢«é€€å›ä¿®æ”¹ã€‚\n"
            
            # ============================================================
            # 4. æœ¬ç¯‡ç‰¹æ®Šè¦æ±‚ï¼ˆç”¨æˆ·è‡ªå®šä¹‰ï¼Œæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            # ============================================================
            custom_requirement = style_profile.get('custom_requirement') if style_profile else None
            if custom_requirement:
                style_instructions += f"\n### â­ æœ¬ç¯‡ç‰¹æ®Šè¦æ±‚ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰\n"
                style_instructions += f"ç”¨æˆ·æ˜ç¡®è¦æ±‚ï¼š{custom_requirement}\n"
                style_instructions += "**å¿…é¡»ä¸¥æ ¼æ‰§è¡Œä¸Šè¿°ç‰¹æ®Šè¦æ±‚ï¼Œä¸å¾—å¿½ç•¥ï¼**\n"
        
        # æ„å»º System Prompt
        system_prompt = f"""{channel_config['system_prompt']['role']}

## âš ï¸ æ ¸å¿ƒçº¦æŸï¼šå­—æ•°è¦æ±‚
- ç›®æ ‡å­—æ•°ï¼š{word_count}å­—
- å…è®¸èŒƒå›´ï¼š{int(word_count * 0.9)}å­— ~ {int(word_count * 1.1)}å­—ï¼ˆÂ±10%åå·®ï¼‰
- ä¼˜å…ˆä¿è¯å†…å®¹å®Œæ•´æ€§å’Œè´¨é‡
{style_instructions}

## âš ï¸ æ ¸å¿ƒçº¦æŸï¼šçœŸå®ç´ æ
- æ–‡ä¸­æ‰€æœ‰æ¡ˆä¾‹ã€æ•…äº‹ã€å¼•ç”¨å¿…é¡»æ¥è‡ªä¸‹æ–¹æä¾›çš„ã€å¯ç”¨ç´ æã€‘
- ä¸¥ç¦å‡­ç©ºç¼–é€ ä»»ä½•æ¡ˆä¾‹æˆ–æ•°æ®
- å¦‚æœç´ æä¸å¤Ÿç”¨ï¼Œè¯·ç®€åŒ–å†…å®¹è€Œä¸æ˜¯æé€ 

## âš ï¸ æ ¸å¿ƒçº¦æŸï¼šç¦ç”¨ä¹¦ç›®ï¼ˆé¿å…AIå‘³ï¼‰
ä¸¾ä¾‹æ—¶ç¦æ­¢ä½¿ç”¨ä»¥ä¸‹è¢«è¿‡åº¦å¼•ç”¨çš„å¸¸è§ä¹¦ç›®ï¼š
ã€Šå¤æ´›çš„ç½‘ã€‹ã€Šå°ç‹å­ã€‹ã€Šçª—è¾¹çš„å°è±†è±†ã€‹ã€Šçˆ±å¿ƒæ ‘ã€‹ã€ŠçŒœçŒœæˆ‘æœ‰å¤šçˆ±ä½ ã€‹ã€Šé€ƒå®¶å°å…”ã€‹ã€Šå¥½é¥¿çš„æ¯›æ¯›è™«ã€‹ã€Šå¤§å«ä¸å¯ä»¥ã€‹
- è¿™äº›ä¹¦ç±å› è¿‡äºç»å…¸å·²æˆä¸ºAIé»˜è®¤ä¸¾ä¾‹ï¼Œå®¹æ˜“è®©å†…å®¹åƒç¯‡ä¸€å¾‹
- å¦‚éœ€ä¹¦ç±ä¸¾ä¾‹ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨ç´ æä¸­æåŠçš„ä¹¦ç›®ï¼Œæˆ–é€‰æ‹©æ›´å°ä¼—ä½†åŒæ ·ä¼˜è´¨çš„ä½œå“

## å†™ä½œé£æ ¼è¦æ±‚
{chr(10).join(['- ' + style for style in channel_config['system_prompt']['writing_style']])}

## ç¦ç”¨è¡¨è¾¾
{', '.join(channel_config['blocked_phrases'])}

## é¢‘é“è§„åˆ™
å¿…é¡»éµå®ˆï¼š{chr(10).join(['- ' + rule for rule in channel_config['channel_specific_rules']['must_do']])}
ä¸¥æ ¼ç¦æ­¢ï¼š{chr(10).join(['- ' + rule for rule in channel_config['channel_specific_rules']['must_not_do']])}
"""
        
        # æ„å»º Think Aloud (v3.5 æ˜¾ç¤ºæ ·æ–‡æ¥æº)
        is_customized = effective_style_profile.get('is_customized', False) if effective_style_profile else False
        custom_req = effective_style_profile.get('custom_requirement', '') if effective_style_profile else ''
        
        think_aloud = f"âœï¸ å¼€å§‹åˆ›ä½œåˆç¨¿ (å•ä¸€æ ‡æ†æ¨¡å¼)...\n\n"
        think_aloud += f"ğŸ“ é¢‘é“ï¼š{channel_config['channel_name']}\n"
        think_aloud += f"ğŸ“ è°ƒæ€§ï¼š{channel_config['brand_personality']}\n"
        think_aloud += f"ğŸ“ å­—æ•°è¦æ±‚ï¼š{word_count}å­—\n"
        think_aloud += f"ğŸ“ é£æ ¼æ¥æºï¼š{style_source}\n"
        
        if is_customized:
            think_aloud += "âš¡ ç”¨æˆ·å·²è‡ªå®šä¹‰é£æ ¼é…ç½®ï¼Œå°†ä¼˜å…ˆæ‰§è¡Œç”¨æˆ·ä¿®æ”¹çš„è§„åˆ™\n"
        if custom_req:
            think_aloud += f"âš¡ ç‰¹æ®Šè¦æ±‚ï¼š{custom_req[:50]}...\n"
        
        think_aloud += "\næ­£åœ¨èå…¥å“ç‰Œé£æ ¼å’ŒçœŸå®ç´ æ..."
        
        user_message = f"""è¯·åˆ›ä½œæ–‡ç« åˆç¨¿ã€‚

## é€‰é¢˜
{selected_topic}

## é£æ ¼æŒ‡å—
{style_guide}

## å¯ç”¨ç´ æï¼ˆåªèƒ½ä½¿ç”¨è¿™äº›ï¼Œç¦æ­¢ç¼–é€ ï¼‰
{materials}

## âš ï¸ åˆ›ä½œè¦æ±‚
1. æ–‡ç« æ€»å­—æ•°ï¼š{int(word_count * 0.9)} ~ {int(word_count * 1.1)} å­—ï¼ˆå…è®¸Â±10%åå·®ï¼‰
2. ä¸¥æ ¼æ¨¡ä»¿é£æ ¼ç”»åƒä¸­çš„å¼€å¤´æ–¹å¼ã€å¥å¼èŠ‚å¥ã€è¯­æ°”ç‰¹ç‚¹
3. çœŸå®ç´ æè‡ªç„¶èå…¥ï¼Œç¦æ­¢å‡­ç©ºç¼–é€ æ¡ˆä¾‹

è¯·å¼€å§‹åˆ›ä½œï¼Œç›´æ¥è¾“å‡ºæ–‡ç« å†…å®¹ã€‚
"""
        
        # æ ¹æ®å­—æ•°åŠ¨æ€è°ƒæ•´ max_tokensï¼ˆä¸­æ–‡çº¦ 1.5 å­—ç¬¦/tokenï¼‰
        estimated_tokens = min(int(word_count * 1.5), 4000)
        
        result = await ai_service.generate_content(
            system_prompt=system_prompt,
            user_message=user_message,
            temperature=0.7,
            max_tokens=estimated_tokens
        )
        
        return {
            "output": result,
            "think_aloud": think_aloud
        }
    
    async def execute_step_8(
        self, 
        draft: str, 
        channel_id: str, 
        word_count: int = 1500,
        style_profile: Dict[str, Any] = None  # é£æ ¼ç”»åƒ
    ) -> Dict[str, Any]:
        """
        Step 8: ä¸‰éå®¡æ ¡æœºåˆ¶ + é£æ ¼ DNA å¯¹é½æ£€æŸ¥
        
        æ–°å¢ï¼šåŸºäºé£æ ¼ç”»åƒçš„è¯„åˆ†æ ‡å‡†æ£€æŸ¥
        """
        channel_config = self.load_channel_config(channel_id)
        blocked_words_config = self.load_blocked_words()
        
        # è®¡ç®—å½“å‰è‰ç¨¿å­—æ•°ï¼ˆå…è®¸ Â±10% åå·®ï¼‰
        current_word_count = len(draft)
        max_allowed = int(word_count * 1.1)  # ä¸Šé™ï¼šç›®æ ‡å­—æ•°çš„110%
        min_allowed = int(word_count * 0.9)  # ä¸‹é™ï¼šç›®æ ‡å­—æ•°çš„90%
        is_over_limit = current_word_count > max_allowed
        
        # æ„å»ºå±è”½è¯åˆ—è¡¨
        blocked_phrases = []
        for category in blocked_words_config['categories'].values():
            for pattern in category['patterns']:
                blocked_phrases.append(f"- {pattern['phrase']} â†’ {pattern['replacement']} ï¼ˆåŸå› ï¼š{pattern['reason']}ï¼‰")
        
        # ================================================================
        # æ„å»ºé£æ ¼ DNA å¯¹é½æ£€æŸ¥æ¸…å•
        # ================================================================
        style_checklist = ""
        if style_profile:
            dims = style_profile.get('stable_features') or style_profile.get('dimensions', {})
            structural_logic = style_profile.get('structural_logic', [])
            writing_guidelines = style_profile.get('writing_guidelines', [])
            
            style_checklist = """
## ğŸ¯ é£æ ¼ DNA å¯¹é½æ£€æŸ¥ï¼ˆå¿…é¡»é€é¡¹æ‰“åˆ†ï¼‰

è¯·å¯¹ç…§ä»¥ä¸‹æ ‡å‡†æ£€æŸ¥æ–‡ç« ï¼Œæ¯é¡¹æ‰“åˆ† âœ“ï¼ˆç¬¦åˆï¼‰æˆ– âœ—ï¼ˆä¸ç¬¦ï¼‰ï¼š

### ç»“æ„æ£€æŸ¥
"""
            # ç»“æ„é€»è¾‘æ£€æŸ¥
            if structural_logic:
                style_checklist += "æ–‡ç« æ®µè½æ˜¯å¦æŒ‰ä»¥ä¸‹é¡ºåºç»„ç»‡ï¼š\n"
                for i, step in enumerate(structural_logic[:6], 1):
                    style_checklist += f"  {i}. [ ] {step}\n"
            
            # å…­ç»´ç‰¹å¾æ£€æŸ¥
            style_checklist += "\n### å…­ç»´ç‰¹å¾æ£€æŸ¥\n"
            
            if dims.get('opening_style'):
                opening = dims['opening_style']
                style_checklist += f"1. [å¼€å¤´] æ˜¯å¦é‡‡ç”¨ã€Œ{opening.get('type', 'æ•…äº‹å¼•å…¥')}ã€æ–¹å¼ï¼Ÿ [ ]\n"
            
            if dims.get('sentence_pattern'):
                sp = dims['sentence_pattern']
                short_ratio = sp.get('short_ratio', 0.6)
                style_checklist += f"2. [å¥å¼] çŸ­å¥å æ¯”æ˜¯å¦ â‰¥ {short_ratio * 100:.0f}%ï¼Ÿ [ ]\n"
                if sp.get('favorite_punctuation'):
                    style_checklist += f"   - æ˜¯å¦ä½¿ç”¨äº†è¿™äº›æ ‡ç‚¹ï¼š{', '.join(sp['favorite_punctuation'][:3])}ï¼Ÿ [ ]\n"
            
            if dims.get('tone'):
                tone = dims['tone']
                style_checklist += f"3. [è¯­æ°”] æ˜¯å¦ä¸ºã€Œ{tone.get('type', 'æ¸©æ¶¦äº²åˆ‡')}ã€é£æ ¼ï¼Ÿ [ ]\n"
                formality = tone.get('formality', 0.3)
                style_checklist += f"   - æ­£å¼åº¦æ˜¯å¦çº¦ {formality * 100:.0f}%ï¼Ÿ [ ]\n"
            
            if dims.get('ending_style'):
                ending = dims['ending_style']
                style_checklist += f"4. [ç»“å°¾] æ˜¯å¦é‡‡ç”¨ã€Œ{ending.get('type', 'å¼•å¯¼æ€è€ƒ')}ã€æ–¹å¼ï¼Ÿ [ ]\n"
            
            if dims.get('expressions'):
                expr = dims['expressions']
                if expr.get('avoid_words'):
                    style_checklist += f"5. [ç¦è¯] æ˜¯å¦ä½¿ç”¨äº†ç¦æ­¢ç”¨è¯ {', '.join(expr['avoid_words'][:5])}ï¼Ÿ [ ] ï¼ˆå¿…é¡»ä¸º âœ—ï¼‰\n"
            
            # åˆ›ä½œæŒ‡å—æ£€æŸ¥
            if writing_guidelines:
                style_checklist += "\n### åˆ›ä½œæŒ‡å—æ£€æŸ¥\n"
                style_checklist += "è¯·é€æ¡æ£€æŸ¥æ˜¯å¦éµå®ˆï¼š\n"
                for i, guideline in enumerate(writing_guidelines[:10], 1):
                    style_checklist += f"  {i}. [ ] {guideline}\n"
            
            style_checklist += """
### å¯¹é½è¯„åˆ†
- æ€»åˆ† = ç¬¦åˆé¡¹æ•° / æ€»é¡¹æ•° Ã— 100%
- åˆæ ¼çº¿ï¼šâ‰¥ 80%
- è‹¥ä¸åˆæ ¼ï¼Œå¿…é¡»ä¿®æ”¹åé‡æ–°è¾“å‡º
"""
        
        # æ ¹æ®æ˜¯å¦è¶…å­—æ•°è°ƒæ•´å®¡æ ¡è¦æ±‚ï¼ˆå…è®¸ Â±10% åå·®ï¼‰
        word_count_instruction = ""
        if is_over_limit:
            word_count_instruction = f"""
## âš ï¸ å­—æ•°è¶…é™è­¦å‘Š
- å½“å‰å­—æ•°ï¼š{current_word_count}å­—
- ç›®æ ‡å­—æ•°ï¼š{word_count}å­—ï¼ˆå…è®¸èŒƒå›´ï¼š{min_allowed}~{max_allowed}å­—ï¼‰
- è¶…å‡ºä¸Šé™ï¼š{current_word_count - max_allowed}å­—
- ã€å¿…é¡»æ‰§è¡Œã€‘åœ¨å®¡æ ¡è¿‡ç¨‹ä¸­ç²¾ç®€å†…å®¹ï¼Œåˆ é™¤å†—ä½™è¡¨è¾¾ï¼Œç¡®ä¿æœ€ç»ˆç‰ˆæœ¬ä¸è¶…è¿‡{max_allowed}å­—
"""
        else:
            word_count_instruction = f"""
## å­—æ•°æ£€æŸ¥
- å½“å‰å­—æ•°ï¼š{current_word_count}å­—
- ç›®æ ‡å­—æ•°ï¼š{word_count}å­—ï¼ˆå…è®¸èŒƒå›´ï¼š{min_allowed}~{max_allowed}å­—ï¼‰
- å­—æ•°ç¬¦åˆè¦æ±‚ âœ“
"""
        
        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å†…å®¹å®¡æ ¡ä¸“å®¶ã€‚è¯·å¯¹æ–‡ç« è¿›è¡Œå››éå®¡æ ¡ã€‚
{word_count_instruction}
{style_checklist}

## ä¸€å®¡ï¼šå†…å®¹å®¡æ ¡
- äº‹å®å‡†ç¡®æ€§
- é€»è¾‘æ¸…æ™°åº¦
- è®ºè¯å……åˆ†æ€§
- æ˜¯å¦æœ‰ç¼–é€ å†…å®¹ï¼ˆä¸¥ç¦ç¼–é€ ï¼ï¼‰

## äºŒå®¡ï¼šé£æ ¼ DNA å¯¹é½ï¼ˆé‡ç‚¹ï¼ï¼‰
å¯¹ç…§ä¸Šæ–¹ã€Šé£æ ¼ DNA å¯¹é½æ£€æŸ¥ã€‹æ¸…å•ï¼Œé€é¡¹æ‰“åˆ†ã€‚
- ä¸ç¬¦åˆçš„é¡¹å¿…é¡»ä¿®æ”¹
- å¯¹é½åˆ†æ•°å¿…é¡» â‰¥ 80%

## ä¸‰å®¡ï¼šé£æ ¼å®¡æ ¡ï¼ˆå»AIå‘³ï¼‰
é¢‘é“è¦æ±‚ï¼š{channel_config['brand_personality']}

å…¨å±€å±è”½è¯ï¼ˆå¿…é¡»æ£€æŸ¥ï¼‰ï¼š
{chr(10).join(blocked_phrases[:20])}  

é¢‘é“å±è”½è¯ï¼š
{', '.join(channel_config['blocked_phrases'])}

## å››å®¡ï¼šç»†èŠ‚æ‰“ç£¨ + å­—æ•°æ§åˆ¶
- å¥å­é•¿åº¦ï¼ˆæ‹†åˆ†è¶…è¿‡40å­—çš„é•¿å¥ï¼‰
- æ®µè½é•¿åº¦ï¼ˆæ¯æ®µä¸è¶…è¿‡200å­—ï¼‰
- æ ‡ç‚¹ç¬¦å·
- è‡ªç„¶è¯­è°ƒ
- æƒ…æ„Ÿå…±é¸£
- ã€é‡è¦ã€‘ç¡®ä¿æ€»å­—æ•°åœ¨{min_allowed}~{max_allowed}å­—èŒƒå›´å†…ï¼ˆÂ±10%åå·®ï¼‰

è¾“å‡ºæ ¼å¼ï¼š
## å®¡æ ¡æŠ¥å‘Š

### é£æ ¼ DNA å¯¹é½è¯„åˆ†
| æ£€æŸ¥é¡¹ | ç»“æœ | è¯´æ˜ |
|-------|------|-----|
| å¼€å¤´æ–¹å¼ | âœ“/âœ— | ... |
| å¥å¼ç‰¹å¾ | âœ“/âœ— | ... |
| è¯­æ°”é£æ ¼ | âœ“/âœ— | ... |
| ç»“å°¾æ–¹å¼ | âœ“/âœ— | ... |
| ç¦è¯æ£€æŸ¥ | âœ“/âœ— | ... |
| åˆ›ä½œæŒ‡å—1 | âœ“/âœ— | ... |
| ... | ... | ... |

**å¯¹é½åˆ†æ•°ï¼šxx%**ï¼ˆ{"éœ€è¦ä¿®æ”¹" if True else "ç¬¦åˆè¦æ±‚"}ï¼‰

### å…¶ä»–é—®é¢˜
1. [å†…å®¹] xxx
2. [é£æ ¼] xxx
3. [ç»†èŠ‚] xxx
4. [å­—æ•°] å½“å‰xxxå­—ï¼Œ{"éœ€è¦ç²¾ç®€" if is_over_limit else "ç¬¦åˆè¦æ±‚"}

### ä¿®æ”¹åç‰ˆæœ¬
ï¼ˆè¾“å‡ºå®Œæ•´çš„ä¿®æ”¹åæ–‡ç« ï¼Œç¡®ä¿å¯¹é½åˆ†æ•° â‰¥ 80% ä¸”å­—æ•°åœ¨{min_allowed}~{max_allowed}å­—èŒƒå›´å†…ï¼‰
"""
        
        think_aloud = f"ğŸ” å¼€å§‹å››éå®¡æ ¡...\n\nå½“å‰å­—æ•°ï¼š{current_word_count}å­—ï¼ˆç›®æ ‡ï¼š{word_count}å­—ï¼Œå…è®¸èŒƒå›´ï¼š{min_allowed}~{max_allowed}å­—ï¼‰\n\nç¬¬ä¸€éï¼šå†…å®¹å®¡æ ¡\nç¬¬äºŒéï¼šé£æ ¼ DNA å¯¹é½æ£€æŸ¥\nç¬¬ä¸‰éï¼šé£æ ¼å®¡æ ¡ï¼ˆå»AIå‘³ï¼‰\nç¬¬å››éï¼šç»†èŠ‚æ‰“ç£¨ + å­—æ•°æ§åˆ¶"
        
        # åŠ¨æ€è°ƒæ•´ max_tokens
        estimated_tokens = min(int(word_count * 2.5), 8000)  # é¢„ç•™å®¡æ ¡æŠ¥å‘Šç©ºé—´
        
        result = await ai_service.generate_content(
            system_prompt=system_prompt,
            user_message=f"è¯·å¯¹ä»¥ä¸‹æ–‡ç« è¿›è¡Œå››éå®¡æ ¡ï¼ˆå­—æ•°å…è®¸èŒƒå›´ï¼š{min_allowed}~{max_allowed}å­—ï¼Œå¯¹é½åˆ†æ•° â‰¥ 80%ï¼‰ï¼š\n\n{draft}",
            temperature=0.3,
            max_tokens=estimated_tokens
        )
        
        return {
            "output": result,
            "think_aloud": think_aloud
        }
    
    async def execute_step_9(self, final_article: str) -> Dict[str, Any]:
        """
        Step 9: æ–‡ç« é…å›¾
        """
        system_prompt = """ä½ æ˜¯é…å›¾æ–¹æ¡ˆä¸“å®¶ã€‚æ ¹æ®æ–‡ç« å†…å®¹ï¼Œæä¾›é…å›¾å»ºè®®ã€‚

è¾“å‡ºæ ¼å¼ï¼š
## é…å›¾æ–¹æ¡ˆ

### å›¾1ï¼šæ ‡é¢˜/ä½ç½®
- æè¿°ï¼šxxx
- é£æ ¼ï¼šæ’ç”»/ç…§ç‰‡/å›¾è¡¨
- AIç»˜å›¾æç¤ºè¯ï¼šxxx

### å›¾2ï¼šæ ‡é¢˜/ä½ç½®
...

## Markdownä»£ç 
```markdown
![å›¾1æè¿°](å›¾ç‰‡è·¯å¾„)

æ–‡ç« å†…å®¹...

![å›¾2æè¿°](å›¾ç‰‡è·¯å¾„)
```

æ³¨æ„ï¼š
- é…å›¾è¦ä¸å†…å®¹ç›¸å…³
- 5-8å¼ ä¸ºå®œ
- æä¾›æ¸…æ™°çš„AIç»˜å›¾æç¤ºè¯
"""
        
        think_aloud = "ğŸ–¼ï¸ æ­£åœ¨ç”Ÿæˆé…å›¾æ–¹æ¡ˆ..."
        
        result = await ai_service.generate_content(
            system_prompt=system_prompt,
            user_message=f"ä¸ºä»¥ä¸‹æ–‡ç« æä¾›é…å›¾æ–¹æ¡ˆï¼š\n\n{final_article[:2000]}...",
            temperature=0.5
        )
        
        return {
            "output": result,
            "think_aloud": think_aloud
        }

# å…¨å±€å·¥ä½œæµå¼•æ“å®ä¾‹
workflow_engine = WorkflowEngine()

