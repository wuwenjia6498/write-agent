"""
å·¥ä½œæµå¼•æ“
å®ç°9æ­¥SOPçš„AIé€»è¾‘ï¼Œæ”¯æŒæ•°æ®åº“æŒä¹…åŒ–å’Œå‘é‡æ£€ç´¢
"""

from typing import Dict, Any, Optional, List
import json
from pathlib import Path
from .ai_service import ai_service
from .db_service import db_service
from .material_processor import process_materials, classify_materials
from .search_service import search_service

class WorkflowEngine:
    """å·¥ä½œæµæ‰§è¡Œå¼•æ“"""
    
    def __init__(self):
        """åˆå§‹åŒ–å·¥ä½œæµå¼•æ“"""
        self.configs_dir = Path(__file__).parent.parent / "configs"
    
    def load_channel_config(self, channel_id: str) -> Dict[str, Any]:
        """
        åŠ è½½é¢‘é“é…ç½®ï¼ˆå•ä¸€æ•°æ®æºç­–ç•¥ï¼‰
        
        è¯»å–ä¼˜å…ˆçº§ï¼š
        1. JSON é…ç½®æ–‡ä»¶ï¼ˆconfigs/channels/{channel_id}.jsonï¼‰- ä¸»æ•°æ®æº
        2. æ•°æ®åº“ channels è¡¨ - ä»…ä½œä¸º fallback
        
        è®¾è®¡åŸåˆ™ï¼š
        - JSON æ–‡ä»¶æ˜¯é…ç½®çš„"çœŸç›¸æ¥æº"ï¼ˆSource of Truthï¼‰
        - æ•°æ®åº“ä¸­çš„ channel_rulesã€blocked_phrases ä»…ç”¨äºç®¡ç†ç•Œé¢å±•ç¤º
        - å·¥ä½œæµæ‰§è¡Œå§‹ç»ˆä»¥ JSON é…ç½®ä¸ºå‡†
        
        Returns:
            Dict[str, Any]: é¢‘é“é…ç½®å­—å…¸
        
        Raises:
            FileNotFoundError: å¦‚æœ JSON æ–‡ä»¶å’Œæ•°æ®åº“éƒ½æ‰¾ä¸åˆ°é…ç½®
        """
        config_file = self.configs_dir / "channels" / f"{channel_id}.json"
        
        # ä¼˜å…ˆçº§ 1: ä» JSON é…ç½®æ–‡ä»¶åŠ è½½
        if config_file.exists():
            with open(config_file, "r", encoding="utf-8") as f:
                config = json.load(f)
                config["_source"] = "json"  # æ ‡è®°æ•°æ®æ¥æºï¼Œä¾¿äºè°ƒè¯•
                return config
        
        # ä¼˜å…ˆçº§ 2: Fallback åˆ°æ•°æ®åº“
        from .db_service import db_service
        channel_data = db_service.get_channel_by_slug(channel_id)
        
        if channel_data:
            # å°†æ•°æ®åº“æ ¼å¼è½¬æ¢ä¸º JSON é…ç½®æ ¼å¼
            config = self._convert_db_to_config_format(channel_data)
            config["_source"] = "database"
            print(f"âš ï¸ è­¦å‘Š: é¢‘é“ '{channel_id}' ä»æ•°æ®åº“åŠ è½½é…ç½®ï¼Œå»ºè®®åˆ›å»º JSON é…ç½®æ–‡ä»¶")
            return config
        
        raise FileNotFoundError(f"é¢‘é“é…ç½®æœªæ‰¾åˆ°: {channel_id}ï¼ˆJSON æ–‡ä»¶å’Œæ•°æ®åº“å‡ä¸å­˜åœ¨ï¼‰")
    
    def _convert_db_to_config_format(self, channel_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å°†æ•°æ®åº“é¢‘é“æ•°æ®è½¬æ¢ä¸º JSON é…ç½®æ ¼å¼
        ç”¨äº fallback åœºæ™¯
        """
        return {
            "channel_id": channel_data.get("slug", ""),
            "channel_name": channel_data.get("name", ""),
            "description": channel_data.get("description", ""),
            "target_audience": channel_data.get("target_audience", ""),
            "brand_personality": channel_data.get("brand_personality", ""),
            "system_prompt": channel_data.get("system_prompt", {
                "role": "ä½ æ˜¯ä¸“ä¸šçš„å†…å®¹åˆ›ä½œä¸“å®¶ã€‚",
                "writing_style": [],
                "tone_guidelines": {}
            }),
            "channel_specific_rules": channel_data.get("channel_rules", {
                "must_do": [],
                "must_not_do": []
            }),
            "blocked_phrases": channel_data.get("blocked_phrases", []),
            "material_tags": channel_data.get("material_tags", []),
            "style_samples": channel_data.get("style_samples", []),
            "style_profile": channel_data.get("style_profile", None)
        }
    
    def sync_channel_config_to_json(self, channel_id: str) -> bool:
        """
        å°†æ•°æ®åº“ä¸­çš„é¢‘é“é…ç½®åŒæ­¥åˆ° JSON æ–‡ä»¶
        
        ç”¨é€”ï¼šç®¡ç†ç•Œé¢æ›´æ–°æ•°æ®åº“åï¼Œè°ƒç”¨æ­¤æ–¹æ³•åŒæ­¥åˆ° JSON
        
        Args:
            channel_id: é¢‘é“ IDï¼ˆslugï¼‰
            
        Returns:
            bool: æ˜¯å¦åŒæ­¥æˆåŠŸ
        """
        from .db_service import db_service
        
        channel_data = db_service.get_channel_by_slug(channel_id)
        if not channel_data:
            print(f"âŒ åŒæ­¥å¤±è´¥: æ•°æ®åº“ä¸­æ‰¾ä¸åˆ°é¢‘é“ '{channel_id}'")
            return False
        
        config = self._convert_db_to_config_format(channel_data)
        config.pop("_source", None)  # ç§»é™¤æ¥æºæ ‡è®°
        
        config_file = self.configs_dir / "channels" / f"{channel_id}.json"
        try:
            with open(config_file, "w", encoding="utf-8") as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            print(f"âœ… é…ç½®å·²åŒæ­¥: {config_file}")
            return True
        except Exception as e:
            print(f"âŒ åŒæ­¥å¤±è´¥: {e}")
            return False
    
    def get_config_source(self, channel_id: str) -> str:
        """
        è·å–é¢‘é“é…ç½®çš„æ•°æ®æ¥æº
        
        Returns:
            str: "json" | "database" | "not_found"
        """
        config_file = self.configs_dir / "channels" / f"{channel_id}.json"
        if config_file.exists():
            return "json"
        
        from .db_service import db_service
        if db_service.get_channel_by_slug(channel_id):
            return "database"
        
        return "not_found"
    
    def load_writing_constraints(self) -> Dict[str, Any]:
        """
        åŠ è½½å…¨å±€å†™ä½œçº¦æŸé…ç½®
        åŒ…å«ï¼šç¦ç”¨ä¹¦ç›®ã€å­—æ•°é™åˆ¶ã€é£æ ¼ DNA åˆæ ¼çº¿ç­‰
        """
        config_file = self.configs_dir / "global" / "writing_constraints.json"
        if config_file.exists():
            with open(config_file, "r", encoding="utf-8") as f:
                return json.load(f)
        else:
            # å…œåº•é»˜è®¤å€¼
            return {
                "banned_books": {
                    "list": [],
                    "replacement_hint": "è¯·é€‰æ‹©å°ä¼—ä½†ä¼˜è´¨çš„ä½œå“"
                },
                "word_count": {"default": 1500, "tolerance": 0.1},
                "sentence": {"max_length": 40},
                "paragraph": {"max_length": 200},
                "style_dna": {"pass_threshold": 0.8}
            }
    
    def load_blocked_words(self) -> Dict[str, Any]:
        """
        åŠ è½½å±è”½è¯åº“
        æ”¯æŒ Markdown è¡¨æ ¼æ ¼å¼å’Œ JSON æ ¼å¼
        """
        # ä¼˜å…ˆä½¿ç”¨ Markdown æ ¼å¼
        md_file = self.configs_dir / "global" / "blocked_words.md"
        json_file = self.configs_dir / "global" / "blocked_words.json"
        
        if md_file.exists():
            return self._parse_blocked_words_markdown(md_file)
        elif json_file.exists():
            with open(json_file, "r", encoding="utf-8") as f:
                return json.load(f)
        else:
            return {"categories": {}}
    
    def _parse_blocked_words_markdown(self, filepath: Path) -> Dict[str, Any]:
        """
        è§£æ Markdown æ ¼å¼çš„å±è”½è¯åº“
        ä»è¡¨æ ¼ä¸­æå–ï¼šç¦ç”¨çŸ­è¯­ã€åŸå› ã€æ›¿æ¢å»ºè®®
        """
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
        
        result = {
            "blocked_words_config": {
                "version": "2.0",
                "description": "Markdown æ ¼å¼å±è”½è¯åº“",
                "format": "markdown"
            },
            "categories": {}
        }
        
        # æŒ‰äºŒçº§æ ‡é¢˜åˆ†å‰²
        import re
        sections = re.split(r'\n## ', content)
        
        for section in sections[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªï¼ˆæ ‡é¢˜å‰çš„å†…å®¹ï¼‰
            lines = section.strip().split('\n')
            if not lines:
                continue
            
            category_name = lines[0].strip()
            
            # è·³è¿‡"å®¡æ ¡æ£€æŸ¥æ¸…å•"ç­‰éè¡¨æ ¼åŒºåŸŸ
            if 'æ£€æŸ¥æ¸…å•' in category_name:
                continue
            
            patterns = []
            in_table = False
            
            for line in lines[1:]:
                line = line.strip()
                
                # è·³è¿‡è¡¨å¤´å’Œåˆ†éš”çº¿
                if line.startswith('| ç¦ç”¨çŸ­è¯­') or line.startswith('|---'):
                    in_table = True
                    continue
                
                # è§£æè¡¨æ ¼è¡Œ
                if in_table and line.startswith('|') and line.endswith('|'):
                    cells = [cell.strip() for cell in line.split('|')[1:-1]]
                    if len(cells) >= 3:
                        patterns.append({
                            "phrase": cells[0],
                            "reason": cells[1],
                            "replacement": cells[2]
                        })
            
            if patterns:
                # ç”Ÿæˆ category key
                category_key = category_name.replace('ï¼ˆ', '_').replace('ï¼‰', '').replace(' ', '_')
                result["categories"][category_key] = {
                    "name": category_name,
                    "patterns": patterns
                }
        
        return result
    
    async def execute_step_1(self, brief: str, channel_id: str) -> Dict[str, Any]:
        """
        Step 1: ç†è§£éœ€æ±‚ & ä¿å­˜Brief
        """
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªéœ€æ±‚åˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æç”¨æˆ·çš„åˆ›ä½œéœ€æ±‚ï¼Œæå–å…³é”®ä¿¡æ¯ã€‚

è¾“å‡ºæ ¼å¼ï¼š
1. ä¸»é¢˜ï¼šxxx
2. ç›®æ ‡è¯»è€…ï¼šxxx
3. æœŸæœ›å­—æ•°ï¼šxxx
4. ç‰¹æ®Šè¦æ±‚ï¼šxxx
5. å…³é”®è¯ï¼šxxx
"""
        
        think_aloud = f"ğŸ“‹ æ­£åœ¨åˆ†æéœ€æ±‚...\nç”¨æˆ·è¾“å…¥ï¼š{brief[:100]}..."
        
        result = await ai_service.generate_content(
            system_prompt=system_prompt,
            user_message=f"è¯·åˆ†æä»¥ä¸‹åˆ›ä½œéœ€æ±‚ï¼š\n\n{brief}",
            temperature=0.3
        )
        
        return {
            "output": result,
            "think_aloud": think_aloud
        }
    
    async def execute_step_2(self, brief_analysis: str, channel_id: str) -> Dict[str, Any]:
        """
        Step 2: ä¿¡æ¯æœç´¢ä¸çŸ¥è¯†ç®¡ç†
        
        æµç¨‹ï¼š
        1. ä½¿ç”¨ Tavily API è¿›è¡ŒçœŸå®ç½‘ç»œæœç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        2. å°†æœç´¢ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ä¼ ç»™ AI
        3. AI åŸºäºçœŸå®æœç´¢ç»“æœç”Ÿæˆè°ƒç ”æŠ¥å‘Š
        4. æç‚¼æ ¸å¿ƒè¦ç‚¹æ‘˜è¦
        """
        think_aloud = "ğŸ” æ­£åœ¨è¿›è¡Œæ·±åº¦è°ƒç ”...\n"
        
        # ä» brief_analysis ä¸­æå–ä¸»é¢˜å…³é”®è¯
        topic_keywords = self._extract_topic_from_brief(brief_analysis)
        think_aloud += f"  - è¯†åˆ«è°ƒç ”ä¸»é¢˜: {topic_keywords}\n"
        
        # ====================================================================
        # é˜¶æ®µé›¶ï¼šçœŸå®ç½‘ç»œæœç´¢ï¼ˆå¦‚æœ Tavily API å¯ç”¨ï¼‰
        # ====================================================================
        search_context = ""
        knowledge_sources = []
        
        if search_service.is_available():
            think_aloud += "  - ğŸŒ æ­£åœ¨è¿›è¡Œç½‘ç»œæœç´¢...\n"
            
            search_result = await search_service.search_for_research(
                topic=topic_keywords,
                context=brief_analysis
            )
            
            if search_result["result_count"] > 0:
                search_context = search_result["context"]
                knowledge_sources = search_result["sources"]
                think_aloud += f"  - âœ“ æœç´¢å®Œæˆï¼Œè·å– {search_result['result_count']} æ¡çœŸå®æ¥æº\n"
            else:
                think_aloud += "  - âš  æœç´¢æ— ç»“æœï¼Œå°†ä½¿ç”¨ AI çŸ¥è¯†åº“ç”Ÿæˆ\n"
        else:
            think_aloud += "  - âš  æœç´¢æœåŠ¡æœªé…ç½®ï¼ˆTAVILY_API_KEYï¼‰ï¼Œä½¿ç”¨ AI çŸ¥è¯†åº“\n"
        
        # ====================================================================
        # é˜¶æ®µä¸€ï¼šç”Ÿæˆè¯¦å°½è°ƒç ”èµ„æ–™
        # ====================================================================
        if search_context:
            # æœ‰æœç´¢ç»“æœï¼ŒåŸºäºçœŸå®æ¥æºç”Ÿæˆ
            research_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å†…å®¹è°ƒç ”ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ã€çœŸå®æœç´¢ç»“æœã€‘ï¼Œæ•´ç†å‡ºç»“æ„åŒ–çš„è°ƒç ”æŠ¥å‘Šã€‚

## é‡è¦è¦æ±‚
1. **å¿…é¡»åŸºäºæœç´¢ç»“æœ**ï¼šåªä½¿ç”¨æœç´¢ç»“æœä¸­çš„ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ 
2. **æ ‡æ³¨æ¥æº**ï¼šåœ¨å…³é”®ä¿¡æ¯åæ ‡æ³¨æ¥æºç¼–å·ï¼Œå¦‚ [æ¥æº1]
3. **ç»“æ„æ¸…æ™°**ï¼šæŒ‰ä¸»é¢˜åˆ†ç±»æ•´ç†

## è¾“å‡ºæ ¼å¼ï¼ˆMarkdownï¼‰

### ä¸€ã€æ ¸å¿ƒæ¦‚å¿µä¸å®šä¹‰
- xxx [æ¥æºX]

### äºŒã€å…³é”®æ•°æ®ä¸äº‹å®
- xxx [æ¥æºX]

### ä¸‰ã€ä¸“å®¶è§‚ç‚¹ä¸ç†è®ºæ”¯æ’‘
- xxx [æ¥æºX]

### å››ã€æ¡ˆä¾‹ä¸å®è¯
- xxx [æ¥æºX]

### äº”ã€å¸¸è§è¯¯åŒºä¸æ³¨æ„äº‹é¡¹
- xxx

### å…­ã€å»¶ä¼¸é˜…è¯»å»ºè®®
- å‚è€ƒæ¥æºä¸­çš„ç›¸å…³é“¾æ¥

---
è¯·åŸºäºçœŸå®æœç´¢ç»“æœç”Ÿæˆè°ƒç ”æŠ¥å‘Šï¼Œç¡®ä¿ä¿¡æ¯å¯æº¯æºã€‚"""
            
            user_message = f"""ã€åˆ›ä½œéœ€æ±‚ã€‘
{brief_analysis}

ã€çœŸå®æœç´¢ç»“æœã€‘
{search_context}

è¯·åŸºäºä»¥ä¸Šæœç´¢ç»“æœï¼Œç”Ÿæˆç»“æ„åŒ–çš„è°ƒç ”æŠ¥å‘Šã€‚"""
        else:
            # æ— æœç´¢ç»“æœï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
            research_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å†…å®¹è°ƒç ”ä¸“å®¶ã€‚è¯·æ ¹æ®åˆ›ä½œéœ€æ±‚ï¼Œè¿›è¡Œå…¨é¢æ·±å…¥çš„èµ„æ–™è°ƒç ”ã€‚

## è°ƒç ”è¦æ±‚
1. **ä¿¡æ¯å…¨é¢**ï¼šè¦†ç›–ä¸»é¢˜çš„å„ä¸ªå…³é”®ç»´åº¦
2. **æœ‰æ®å¯æŸ¥**ï¼šæ ‡æ³¨ä¿¡æ¯æ¥æºç±»å‹ï¼ˆå­¦æœ¯ç ”ç©¶/å®˜æ–¹æ•°æ®/ä¸“å®¶è§‚ç‚¹/æ¡ˆä¾‹å®è¯ï¼‰
3. **å®ç”¨å¯¼å‘**ï¼šèšç„¦å¯¹åˆ›ä½œæœ‰å®é™…ä»·å€¼çš„ä¿¡æ¯
4. **ç»“æ„æ¸…æ™°**ï¼šåˆ†ç±»æ•´ç†ï¼Œä¾¿äºåç»­å¼•ç”¨

## è¾“å‡ºæ ¼å¼ï¼ˆMarkdownï¼‰

### ä¸€ã€æ ¸å¿ƒæ¦‚å¿µä¸å®šä¹‰
- xxx

### äºŒã€å…³é”®æ•°æ®ä¸äº‹å®
- xxxï¼ˆæ¥æºï¼šxxxï¼‰

### ä¸‰ã€ä¸“å®¶è§‚ç‚¹ä¸ç†è®ºæ”¯æ’‘
- xxx

### å››ã€æ¡ˆä¾‹ä¸å®è¯
- xxx

### äº”ã€å¸¸è§è¯¯åŒºä¸æ³¨æ„äº‹é¡¹
- xxx

### å…­ã€å»¶ä¼¸é˜…è¯»å»ºè®®
- xxx

---
è¯·ç¡®ä¿å†…å®¹è¯¦å®ã€æœ‰æ·±åº¦ï¼Œä¸ºåç»­åˆ›ä½œæä¾›å……è¶³çš„ç´ ææ”¯æ’‘ã€‚"""
            
            user_message = f"è¯·æ ¹æ®ä»¥ä¸‹éœ€æ±‚åˆ†æè¿›è¡Œæ·±åº¦è°ƒç ”ï¼š\n\n{brief_analysis}"
        
        think_aloud += "  - æ­£åœ¨ç”Ÿæˆè¯¦å°½è°ƒç ”èµ„æ–™...\n"
        
        knowledge_base = await ai_service.generate_content(
            system_prompt=research_prompt,
            user_message=user_message,
            temperature=0.4,
            max_tokens=4000
        )
        
        think_aloud += f"  - âœ“ è°ƒç ”èµ„æ–™ç”Ÿæˆå®Œæˆ ({len(knowledge_base)} å­—)\n"
        
        # ====================================================================
        # é˜¶æ®µäºŒï¼šæç‚¼æ ¸å¿ƒè¦ç‚¹æ‘˜è¦ï¼ˆ300å­—ä»¥å†…ï¼‰
        # ====================================================================
        summary_prompt = """ä½ æ˜¯ä¸€ä½æ“…é•¿ä¿¡æ¯æç‚¼çš„ç¼–è¾‘ã€‚è¯·å°†è°ƒç ”èµ„æ–™æç‚¼ä¸ºç®€æ´çš„æ ¸å¿ƒè¦ç‚¹ã€‚

ã€ä¸¥æ ¼è¦æ±‚ã€‘
- æ€»å­—æ•°ï¼š200-300å­—
- è¦ç‚¹æ•°ï¼š3-5ä¸ªæ ¸å¿ƒå‘ç°
- æ ¼å¼ï¼šçº¯æ–‡æœ¬ï¼Œç¦æ­¢ä½¿ç”¨ä»»ä½• Markdown æˆ– HTML æ ‡ç­¾

ã€è¾“å‡ºæ ¼å¼ç¤ºä¾‹ã€‘
æ ¸å¿ƒå‘ç°ï¼š
1. ç¬¬ä¸€ä¸ªè¦ç‚¹çš„ä¸€å¥è¯æ¦‚æ‹¬
2. ç¬¬äºŒä¸ªè¦ç‚¹çš„ä¸€å¥è¯æ¦‚æ‹¬
3. ç¬¬ä¸‰ä¸ªè¦ç‚¹çš„ä¸€å¥è¯æ¦‚æ‹¬

åˆ›ä½œå»ºè®®ï¼šä¸€å¥è¯è¯´æ˜è¿™äº›å‘ç°å¯¹æ–‡ç« åˆ›ä½œçš„æŒ‡å¯¼æ„ä¹‰ã€‚

ã€ç¦æ­¢äº‹é¡¹ã€‘
- ä¸è¦ä½¿ç”¨ # ## ### ç­‰æ ‡é¢˜ç¬¦å·
- ä¸è¦ä½¿ç”¨ ** __ ç­‰åŠ ç²—ç¬¦å·
- ä¸è¦ä½¿ç”¨ <strong> <b> ç­‰ HTML æ ‡ç­¾
- ç›´æ¥è¾“å‡ºçº¯æ–‡æœ¬å³å¯"""
        
        think_aloud += "  - æ­£åœ¨æç‚¼æ ¸å¿ƒè¦ç‚¹æ‘˜è¦...\n"
        
        knowledge_summary = await ai_service.generate_content(
            system_prompt=summary_prompt,
            user_message=f"è¯·å°†ä»¥ä¸‹è°ƒç ”èµ„æ–™æç‚¼ä¸º 300 å­—ä»¥å†…çš„æ ¸å¿ƒè¦ç‚¹æ‘˜è¦ï¼š\n\n{knowledge_base}",
            temperature=0.3,
            max_tokens=500
        )
        
        think_aloud += "  - âœ“ æ‘˜è¦æç‚¼å®Œæˆ\n"
        think_aloud += "\nğŸ“š è°ƒç ”é˜¶æ®µå®Œæˆï¼Œè¯·å®¡é˜…å¹¶ç¡®è®¤è°ƒç ”ç»“è®ºã€‚"
        
        return {
            "output": knowledge_base,           # å®Œæ•´è°ƒç ”èµ„æ–™
            "knowledge_summary": knowledge_summary,  # æ ¸å¿ƒè¦ç‚¹æ‘˜è¦
            "knowledge_sources": knowledge_sources,  # çœŸå®æœç´¢æ¥æº
            "think_aloud": think_aloud,
            "is_checkpoint": True  # è®¾ä¸ºå¡ç‚¹ï¼Œéœ€ç”¨æˆ·ç¡®è®¤
        }
    
    async def execute_step_3(self, brief_analysis: str, channel_id: str) -> Dict[str, Any]:
        """
        Step 3: é€‰é¢˜è®¨è®ºï¼ˆå¿…åšå¡ç‚¹ï¼‰
        """
        channel_config = self.load_channel_config(channel_id)
        writing_constraints = self.load_writing_constraints()
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½ç¦ç”¨ä¹¦ç›®
        banned_books = writing_constraints.get('banned_books', {})
        banned_books_list = ''.join(banned_books.get('list', []))
        banned_books_hint = banned_books.get('replacement_hint', 'è¯·é€‰æ‹©æ›´å°ä¼—ä½†åŒæ ·ä¼˜è´¨çš„ä½œå“')
        
        system_prompt = f"""{channel_config['system_prompt']['role']}

è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚æä¾›3-4ä¸ªé€‰é¢˜æ–¹å‘ã€‚

æ¯ä¸ªé€‰é¢˜åŒ…å«ï¼š
1. æ ‡é¢˜ï¼ˆå¸å¼•äººä½†ä¸æ ‡é¢˜å…šï¼‰
2. æ ¸å¿ƒè§‚ç‚¹
3. å¤§çº²ï¼ˆ3-5ä¸ªè¦ç‚¹ï¼‰
4. é¢„ä¼°å·¥ä½œé‡ï¼ˆå­—æ•°ã€æ‰€éœ€ç´ æï¼‰
5. ä¼˜åŠ£åˆ†æ

## âš ï¸ ç¦ç”¨ä¹¦ç›®ï¼ˆé¿å…AIå‘³ï¼‰
ä¸¾ä¾‹æ—¶ç¦æ­¢ä½¿ç”¨ä»¥ä¸‹è¢«è¿‡åº¦å¼•ç”¨çš„å¸¸è§ä¹¦ç›®ï¼š
{banned_books_list}
- {banned_books_hint}

å†™ä½œé£æ ¼è¦æ±‚ï¼š
{chr(10).join(['- ' + style for style in channel_config['system_prompt']['writing_style']])}
"""
        
        think_aloud = f"ğŸ’¡ æ­£åœ¨ä¸º'{channel_config['channel_name']}'é¢‘é“ç”Ÿæˆé€‰é¢˜æ–¹æ¡ˆ...\n\næ€è€ƒé‡ç‚¹ï¼š\n- ç¬¦åˆé¢‘é“è°ƒæ€§\n- é¿å…ç©ºæ´å¥—è¯\n- æœ‰ç‹¬ç‰¹è§†è§’"
        
        result = await ai_service.generate_content(
            system_prompt=system_prompt,
            user_message=f"åŸºäºä»¥ä¸‹éœ€æ±‚åˆ†æï¼Œæä¾›3-4ä¸ªé€‰é¢˜æ–¹å‘ï¼š\n\n{brief_analysis}",
            temperature=0.8,
            max_tokens=6000
        )
        
        return {
            "output": result,
            "think_aloud": think_aloud,
            "is_checkpoint": True  # å¡ç‚¹ï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤
        }
    
    async def execute_step_4(self, selected_topic: str) -> Dict[str, Any]:
        """
        Step 4: åˆ›å»ºåä½œæ–‡æ¡£
        """
        system_prompt = """ä½ æ˜¯é¡¹ç›®ç®¡ç†ä¸“å®¶ã€‚æ ¹æ®é€‰å®šçš„é€‰é¢˜ï¼Œåˆ›å»ºåä½œæ¸…å•ã€‚

è¾“å‡ºæ ¼å¼ï¼š
## AIè´Ÿè´£çš„ä»»åŠ¡
- [ ] ä»»åŠ¡1
- [ ] ä»»åŠ¡2

## ç”¨æˆ·éœ€è¦æä¾›çš„å†…å®¹
- [ ] çœŸå®æ¡ˆä¾‹ï¼šxxx
- [ ] ä¸ªäººè§‚ç‚¹ï¼šxxx
- [ ] æ•°æ®æ”¯æŒï¼šxxx

## æ³¨æ„äº‹é¡¹
- ä¸ç¼–é€ æ•°æ®
- ä¸ä½¿ç”¨å¥—è¯
"""
        
        think_aloud = "ğŸ“ æ­£åœ¨ç”Ÿæˆåä½œæ¸…å•..."
        
        result = await ai_service.generate_content(
            system_prompt=system_prompt,
            user_message=f"ä¸ºä»¥ä¸‹é€‰é¢˜åˆ›å»ºåä½œæ¸…å•ï¼š\n\n{selected_topic}",
            temperature=0.3
        )
        
        return {
            "output": result,
            "think_aloud": think_aloud
        }
    
    async def execute_step_5(
        self, 
        selected_topic: str, 
        channel_id: str,
        task_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Step 5: é£æ ¼å»ºæ¨¡ä¸ç´ ææ£€ç´¢ (v3.5 - æ ·æ–‡çŸ©é˜µæ¨¡å¼)
        
        æ ¸å¿ƒå˜åŒ–ï¼š
        1. åˆ‡æ¢ä¸º"æ ·æ–‡çŸ©é˜µ"æ¨¡å¼ï¼Œæ¯ç¯‡æ ·æ–‡ç‹¬ç«‹ä¿æŒ 6 ç»´ç‰¹å¾
        2. æ™ºèƒ½æ¨èæœ€åŒ¹é…çš„å•ç¯‡æ ·æ–‡ï¼ˆåŸºäº custom_tags + Brief å…³é”®è¯ï¼‰
        3. ä¸»ç¼–å¯åœ¨å‰ç«¯é€‰æ‹©/åˆ‡æ¢å‚è€ƒæ ·æ–‡
        4. Step 7 å°†ä½¿ç”¨æ‰€é€‰æ ·æ–‡çš„ç‹¬ç«‹ style_profile
        """
        channel_config = self.load_channel_config(channel_id)
        
        think_aloud = "[Step 5] å¼€å§‹é£æ ¼å»ºæ¨¡ä¸ç´ ææ£€ç´¢ (æ ·æ–‡çŸ©é˜µæ¨¡å¼)...\n"
        
        # è·å–é¢‘é“æ•°æ®
        channel_data = db_service.get_channel_by_slug(channel_id)
        
        # ====================================================================
        # 1. æ ·æ–‡çŸ©é˜µï¼šè·å–æ‰€æœ‰å·²åˆ†æçš„æ ·æ–‡ï¼Œæ™ºèƒ½æ¨èæœ€åŒ¹é…çš„ä¸€ç¯‡
        # ====================================================================
        think_aloud += "\n[æ ·æ–‡çŸ©é˜µ] æ­£åœ¨åŠ è½½æ ·æ–‡åº“...\n"
        
        style_profile = None
        selected_sample = None
        all_samples = []
        
        # v3.5: ä»ç‹¬ç«‹è¡¨è·å–æ ·æ–‡
        if channel_data:
            # æå–å…³é”®è¯ç”¨äºåŒ¹é…
            keywords = self._extract_keywords(selected_topic)
            think_aloud += f"  - é€‰é¢˜å…³é”®è¯: {', '.join(keywords[:5])}\n"
            
            # è·å–æ ·æ–‡å¹¶è®¡ç®—åŒ¹é…åˆ†æ•°
            all_samples = db_service.get_style_samples_for_matching(
                channel_id=channel_data['id'],
                keywords=keywords
            )
            
            if all_samples:
                think_aloud += f"  - âœ“ æ‰¾åˆ° {len(all_samples)} ç¯‡å·²åˆ†æçš„æ ·æ–‡\n"
                
                # æ¨èåŒ¹é…åº¦æœ€é«˜çš„æ ·æ–‡
                selected_sample = all_samples[0]
                matched_tags = selected_sample.get('matched_tags', [])
                
                think_aloud += f"\n[æ™ºèƒ½æ¨è] æœ€ä½³åŒ¹é…æ ·æ–‡ï¼šã€Š{selected_sample['title']}ã€‹\n"
                if matched_tags:
                    think_aloud += f"  - åŒ¹é…æ ‡ç­¾: {', '.join(matched_tags)}\n"
                think_aloud += f"  - åŒ¹é…åˆ†æ•°: {selected_sample.get('match_score', 0)}\n"
                
                # ä½¿ç”¨æ¨èæ ·æ–‡çš„ style_profile ä½œä¸ºé»˜è®¤
                style_profile = selected_sample.get('style_profile', {})
                
                # å±•ç¤ºè¯¥æ ·æ–‡çš„ 6 ç»´ç‰¹å¾æ‘˜è¦
                if style_profile:
                    dims = style_profile
                    think_aloud += "  - 6 ç»´ç‰¹å¾:\n"
                    if dims.get('opening_style'):
                        think_aloud += f"    Â· å¼€å¤´: {dims['opening_style'].get('type', '-')}\n"
                    if dims.get('tone'):
                        think_aloud += f"    Â· è¯­æ°”: {dims['tone'].get('type', '-')}\n"
                    if dims.get('ending_style'):
                        think_aloud += f"    Â· ç»“å°¾: {dims['ending_style'].get('type', '-')}\n"
            else:
                think_aloud += "  - âš  è¯¥é¢‘é“æš‚æ— å·²åˆ†æçš„æ ·æ–‡\n"
        
        # å›é€€ï¼šå¦‚æœæ²¡æœ‰ç‹¬ç«‹è¡¨æ ·æ–‡ï¼Œå°è¯•ä»æ—§ JSONB å­—æ®µè·å–
        if not style_profile:
            style_samples = channel_data.get('style_samples', []) if channel_data else []
            
            if channel_data and channel_data.get('style_profile'):
                style_profile = channel_data['style_profile']
                think_aloud += f"  - å›é€€åˆ°é¢‘é“æ•´ä½“é£æ ¼ç”»åƒ\n"
            elif style_samples:
                # ä½¿ç”¨ç¬¬ä¸€ç¯‡æ ·æ–‡çš„ç‰¹å¾
                for sample in style_samples:
                    if sample.get('features'):
                        style_profile = sample['features']
                        think_aloud += f"  - ä½¿ç”¨æ ·æ–‡ã€Š{sample.get('title', 'æ— æ ‡é¢˜')}ã€‹çš„ç‰¹å¾\n"
                        break
        
        # æœ€ç»ˆå›é€€ï¼šé»˜è®¤é£æ ¼
        if not style_profile:
            think_aloud += "  - âš  ä½¿ç”¨é»˜è®¤é£æ ¼é…ç½®\n"
            style_profile = {
                "style_portrait": "ä¸“ä¸šè€Œäº²åˆ‡çš„å†…å®¹åˆ›ä½œè€…ï¼Œç”¨çœŸè¯šçš„æ€åº¦åˆ†äº«è§‚ç‚¹",
                "structural_logic": ["åœºæ™¯åˆ‡å…¥", "é—®é¢˜å¼•å‡º", "è§‚ç‚¹å±•å¼€", "æ¡ˆä¾‹æ”¯æ’‘", "æ€»ç»“å‡å"],
                "tone_features": ["çœŸè¯š", "ä¸“ä¸š", "äº²åˆ‡"],
                "opening_style": {"type": "story_intro", "description": "å»ºè®®ç”¨ç”Ÿæ´»åœºæ™¯å¼•å…¥"},
                "tone": {"type": "warm_friend", "formality": 0.3, "description": "æ¸©æ¶¦äº²åˆ‡ï¼Œåƒæœ‹å‹èŠå¤©"},
                "ending_style": {"type": "reflection", "description": "å¼•å¯¼è¯»è€…æ€è€ƒ"},
                "writing_guidelines": ["é¿å…è¯´æ•™è¯­æ°”", "å¤šç”¨çŸ­å¥", "èå…¥çœŸå®ç»å†"]
            }
        
        # ====================================================================
        # 2. ä»æ•°æ®åº“æ£€ç´¢çœŸå®ç´ æ
        # ====================================================================
        think_aloud += "\n[RAG] æ­£åœ¨ä»ç´ æåº“æ£€ç´¢ç›¸å…³ç´ æ...\n"
        think_aloud += f"  - é¢‘é“è¿‡æ»¤: {channel_id}\n"
        
        retrieved_materials = []
        raw_materials = []
        
        if channel_data:
            keywords = self._extract_keywords(selected_topic)
            think_aloud += f"  - æ£€ç´¢å…³é”®è¯: {', '.join(keywords)}\n"
            
            # å¢å¤§æ£€ç´¢é‡ï¼Œä¸ºå»é‡ç•™ä½™é‡
            raw_materials = db_service.search_materials_by_keywords(
                channel_id=channel_data["id"],
                keywords=keywords,
                limit=15
            )
            think_aloud += f"  - åŸå§‹æ£€ç´¢: {len(raw_materials)} æ¡ç´ æ\n"
        
        if not raw_materials and channel_data:
            think_aloud += "  - æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œè·å–é¢‘é“é€šç”¨ç´ æ...\n"
            raw_materials = db_service.get_materials_by_channel(
                channel_id=channel_data["id"],
                limit=15
            )
        
        # ====================================================================
        # 2.1 ç´ ææ¸…æ´—ä¸å»é‡
        # ====================================================================
        if raw_materials:
            think_aloud += "\n[ç´ æå¤„ç†] å¼€å§‹æ¸…æ´—ä¸å»é‡...\n"
            original_count = len(raw_materials)
            
            # è°ƒç”¨ç´ æå¤„ç†å™¨ï¼šå™ªå£°è¿‡æ»¤ + æ¥æºå»é‡ + å†…å®¹å»é‡
            retrieved_materials = process_materials(
                raw_materials,
                enable_spam_filter=True,
                enable_source_dedupe=True,
                enable_content_dedupe=True,
                content_similarity_threshold=0.85
            )
            
            # é™åˆ¶æœ€ç»ˆæ•°é‡
            retrieved_materials = retrieved_materials[:8]
            
            removed_count = original_count - len(retrieved_materials)
            if removed_count > 0:
                think_aloud += f"  - æ¸…æ´—å®Œæˆ: {original_count} -> {len(retrieved_materials)} æ¡\n"
                think_aloud += f"  - ç§»é™¤ {removed_count} æ¡é‡å¤/è¥é”€å†…å®¹\n"
            else:
                think_aloud += f"  - æ¸…æ´—å®Œæˆ: {len(retrieved_materials)} æ¡æœ‰æ•ˆç´ æ\n"
        
        # ====================================================================
        # 2.2 ç´ æåˆ†ç±»ï¼ˆé•¿æ–‡ vs çŸ­ç¢ï¼‰
        # ====================================================================
        classified_materials = {"long": [], "short": []}
        if retrieved_materials:
            classified_materials = classify_materials(retrieved_materials, long_threshold=200)
            think_aloud += f"  - åˆ†ç±»: {len(classified_materials['long'])} æ¡é•¿æ–‡ + {len(classified_materials['short'])} æ¡çµæ„Ÿç¢ç‰‡\n"
        
        # ====================================================================
        # 2.3 é•¿æ–‡ç´ ææ‘˜è¦åŒ–ï¼ˆv3.7 æ–°å¢ï¼‰
        # ====================================================================
        if classified_materials['long']:
            think_aloud += "\n[ç´ ææ‘˜è¦] æ­£åœ¨åˆ†æé•¿æ–‡ç´ æ...\n"
            summarized_long = []
            for mat in classified_materials['long']:
                content_len = len(mat.get('content', ''))
                if content_len >= 500:
                    think_aloud += f"  - åˆ†æã€Š{mat.get('source', 'æœªå‘½å')}ã€‹({content_len}å­—)...\n"
                    summarized_mat = await self._summarize_material(mat, selected_topic)
                    summarized_long.append(summarized_mat)
                else:
                    mat['is_summarized'] = False
                    summarized_long.append(mat)
            classified_materials['long'] = summarized_long
            think_aloud += f"  - âœ“ å®Œæˆ {len(summarized_long)} æ¡é•¿æ–‡æ‘˜è¦\n"
        
        # ====================================================================
        # 3. æ ¼å¼åŒ–è¾“å‡ºï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨æ‘˜è¦è€Œéå…¨æ–‡ï¼‰
        # ====================================================================
        materials_context = ""
        if retrieved_materials:
            materials_context = "\n\n## ä»ç´ æåº“æ£€ç´¢åˆ°çš„çœŸå®ç´ æ\n"
            materials_context += "ï¼ˆä»¥ä¸‹ç´ ææ¥è‡ª15å¹´ç§¯ç´¯çš„çœŸå®ç»å†ï¼Œè¯·åœ¨åˆ›ä½œä¸­è¿ç”¨è¿™äº›çœŸå®æ¡ˆä¾‹å’Œè§‚ç‚¹ï¼‰\n\n"
            
            # é•¿æ–‡ç´ æï¼šä½¿ç”¨æ‘˜è¦
            if classified_materials['long']:
                materials_context += "### ã€é•¿æ–‡ç´ æã€‘\n"
                for i, mat in enumerate(classified_materials['long'], 1):
                    source_name = mat.get('source', f"ç´ æ{i}")
                    materials_context += f"\n**{i}. [{mat['material_type']}] {source_name}**\n"
                    
                    # ä¼˜å…ˆä½¿ç”¨ AI æ‘˜è¦
                    if mat.get('ai_summary'):
                        materials_context += f"{mat['ai_summary']}\n"
                    elif mat.get('summary'):
                        materials_context += f"{mat['summary']}\n"
                    else:
                        # å›é€€ï¼šæˆªå–å‰300å­—
                        content = mat.get('content', '')
                        materials_context += f"{content[:300]}{'...' if len(content) > 300 else ''}\n"
                    
                    # æ˜¾ç¤ºå…³é”®è¦ç‚¹
                    if mat.get('key_points'):
                        materials_context += "**å…³é”®è¦ç‚¹**ï¼š" + " | ".join(mat['key_points']) + "\n"
                materials_context += "\n"
            
            # çŸ­ç¢ç´ æï¼šç›´æ¥æ˜¾ç¤º
            if classified_materials['short']:
                materials_context += "### ã€çµæ„Ÿç¢ç‰‡ã€‘\n"
                for mat in classified_materials['short']:
                    materials_context += f"- [{mat['material_type']}] {mat['content']}\n"
                materials_context += "\n"
            
            think_aloud += f"\n[RAG] å·²å¤„ç† {len(retrieved_materials)} æ¡ç´ æï¼ˆ{len(classified_materials['long'])} é•¿æ–‡ + {len(classified_materials['short'])} ç¢ç‰‡ï¼‰\n"
        else:
            think_aloud += "\n[WARN] ç´ æåº“ä¸­æš‚æ— ç›¸å…³ç´ æï¼Œè¯·åœ¨åˆ›ä½œæ—¶æ³¨å…¥çœŸå®ç»å†\n"
        
        # ====================================================================
        # 4. ç”Ÿæˆé£æ ¼æŒ‡å¯¼æ–‡æ¡£ (v3.5 ç®€åŒ–ï¼Œä¸å†æ˜¾ç¤º JSON)
        # ====================================================================
        
        # æ„å»ºé£æ ¼æ‘˜è¦ï¼ˆå» JSON åŒ–ï¼‰
        style_summary = ""
        if style_profile:
            if style_profile.get('style_portrait'):
                style_summary += f"**é£æ ¼ç”»åƒ**ï¼š{style_profile['style_portrait']}\n\n"
            
            if style_profile.get('structural_logic'):
                logic = style_profile['structural_logic']
                style_summary += f"**ç»“æ„é€»è¾‘**ï¼š{' â†’ '.join(logic[:5])}\n\n"
            
            if style_profile.get('tone_features'):
                style_summary += f"**è¯­æ°”ç‰¹å¾**ï¼š{', '.join(style_profile['tone_features'][:4])}\n\n"
            
            if style_profile.get('writing_guidelines'):
                guidelines = style_profile['writing_guidelines']
                style_summary += "**åˆ›ä½œæŒ‡å—**ï¼š\n"
                for i, g in enumerate(guidelines[:5], 1):
                    style_summary += f"  {i}. {g}\n"
        
        sample_info = ""
        if selected_sample:
            sample_info = f"""
## æ¨èå‚è€ƒæ ·æ–‡
- **æ ‡é¢˜**ï¼šã€Š{selected_sample['title']}ã€‹
- **æ ‡ç­¾**ï¼š{', '.join(selected_sample.get('custom_tags', []) or ['æ— æ ‡ç­¾'])}
- **åŒ¹é…åº¦**ï¼š{selected_sample.get('match_score', 0)} åˆ†

> æœ¬æ¬¡åˆ›ä½œå°†å‚è€ƒæ­¤æ ·æ–‡çš„å†™ä½œèŒƒå¼ã€‚å¦‚éœ€æ›´æ¢ï¼Œè¯·åœ¨å·¥ä½œå°é€‰æ‹©å…¶ä»–æ ·æ–‡ã€‚
"""
        
        style_guide = f"""## æœ¬ç¯‡åˆ›ä½œé£æ ¼æŒ‡å¼•

{style_summary}
{sample_info}

## åˆ›ä½œè¦æ±‚
1. **ä¸¥æ ¼æ¨¡ä»¿æ‰€é€‰æ ·æ–‡çš„å†™ä½œèŒƒå¼**ï¼ˆå¼€å¤´æ–¹å¼ã€å¥å¼èŠ‚å¥ã€è¯­æ°”ç‰¹ç‚¹ã€ç»“å°¾é£æ ¼ï¼‰
2. **çœŸå®ç´ æä¼˜å…ˆ**ï¼šå°†æ£€ç´¢åˆ°çš„çœŸå®ç»å†è‡ªç„¶èå…¥ï¼Œç¦æ­¢å‡­ç©ºç¼–é€ æ¡ˆä¾‹
3. **ä¿æŒé¢‘é“è°ƒæ€§**ï¼š{channel_config.get('brand_personality', 'æ¸©æ¶¦ã€ä¸“ä¸šã€æœ‰æ·±åº¦')}

{materials_context}
"""
        
        # æŒä¹…åŒ– Think Aloud
        if task_id:
            db_service.add_think_aloud_log(task_id, 5, think_aloud)
        
        return {
            "output": style_guide,
            "think_aloud": think_aloud,
            "retrieved_materials": retrieved_materials,
            "classified_materials": classified_materials,
            "style_profile": style_profile,
            # v3.5 æ–°å¢ï¼šæ ·æ–‡æ¨èæ•°æ®
            "selected_sample": selected_sample,
            "all_samples": all_samples,
            "has_sample_recommendation": bool(selected_sample)
        }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰"""
        # ç§»é™¤å¸¸è§åœç”¨è¯ï¼Œæå–å…³é”®è¯
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'å’Œ', 'äº†', 'ä¸', 'å¯¹', 'ä¸º', 'ä»¥', 'ç­‰', 
                      'è¿™', 'é‚£', 'å°±', 'ä¹Ÿ', 'éƒ½', 'è¦', 'èƒ½', 'ä¼š', 'å¯ä»¥', 'åº”è¯¥',
                      'ä¸€ä¸ª', 'æˆ‘ä»¬', 'ä»–ä»¬', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ'}
        
        # ç®€å•åˆ†è¯ï¼ˆæŒ‰æ ‡ç‚¹å’Œç©ºæ ¼ï¼‰
        import re
        words = re.split(r'[ï¼Œã€‚ã€ï¼ï¼Ÿï¼šï¼›""''ï¼ˆï¼‰\s]+', text)
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        keywords = [
            w.strip() for w in words 
            if w.strip() and len(w.strip()) >= 2 and w.strip() not in stop_words
        ]
        
        # å–å‰10ä¸ªå…³é”®è¯
        return keywords[:10]
    
    async def _summarize_material(self, material: Dict[str, Any], topic: str) -> Dict[str, Any]:
        """
        å¯¹é•¿æ–‡ç´ æç”Ÿæˆæ‘˜è¦å’Œå…³é”®ç‚¹ï¼ˆv3.7 æ–°å¢ï¼‰
        
        åŠŸèƒ½ï¼š
        1. æå–æ ¸å¿ƒè®ºç‚¹å’Œå…³é”®è§‚ç‚¹
        2. è¯†åˆ«å¯å¼•ç”¨çš„å…·ä½“æ¡ˆä¾‹/æ•°æ®
        3. ç”Ÿæˆç®€æ´çš„æ‘˜è¦ï¼ˆä¾¿äº AI ç†è§£å’Œè¿ç”¨ï¼‰
        
        Args:
            material: ç´ æå­—å…¸ï¼ŒåŒ…å« content, material_type, source ç­‰
            topic: å½“å‰åˆ›ä½œçš„é€‰é¢˜ï¼Œç”¨äºå…³è”æ€§åˆ†æ
            
        Returns:
            åŒ…å«æ‘˜è¦ä¿¡æ¯çš„ç´ æå­—å…¸
        """
        content = material.get('content', '')
        material_type = material.get('material_type', 'å…¶ä»–')
        source = material.get('source', '')
        
        # åªå¯¹è¶…è¿‡ 500 å­—çš„é•¿æ–‡ç´ æç”Ÿæˆæ‘˜è¦
        if len(content) < 500:
            material['summary'] = content[:200] + '...' if len(content) > 200 else content
            material['key_points'] = []
            return material
        
        # æ„å»ºæ‘˜è¦æå– Prompt
        summary_prompt = f"""è¯·åˆ†æä»¥ä¸‹{material_type}ç´ æï¼Œæå–ä¸å½“å‰é€‰é¢˜ç›¸å…³çš„æ ¸å¿ƒä¿¡æ¯ã€‚

ã€å½“å‰é€‰é¢˜ã€‘
{topic}

ã€ç´ æå†…å®¹ã€‘ï¼ˆ{len(content)}å­—ï¼‰
{content[:3000]}  # é™åˆ¶è¾“å…¥é•¿åº¦

ã€è¾“å‡ºè¦æ±‚ã€‘
è¯·ç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆæ¯é¡¹ä¸è¶…è¿‡50å­—ï¼‰ï¼š

**æ ¸å¿ƒè§‚ç‚¹**ï¼šï¼ˆä¸€å¥è¯æ¦‚æ‹¬è¯¥ç´ æçš„æ ¸å¿ƒè®ºç‚¹ï¼‰

**å…³é”®è¦ç‚¹**ï¼š
1. ï¼ˆè¦ç‚¹1ï¼‰
2. ï¼ˆè¦ç‚¹2ï¼‰
3. ï¼ˆè¦ç‚¹3ï¼‰

**å¯å¼•ç”¨å†…å®¹**ï¼šï¼ˆå¦‚æœ‰å…·ä½“æ¡ˆä¾‹ã€æ•°æ®ã€é‡‘å¥ï¼Œåˆ—å‡º1-2æ¡æœ€æœ‰ä»·å€¼çš„ï¼‰

**ä¸é€‰é¢˜å…³è”**ï¼šï¼ˆè¯´æ˜è¯¥ç´ æå¦‚ä½•æœåŠ¡äºå½“å‰é€‰é¢˜ï¼‰"""

        try:
            summary_result = await ai_service.generate_content(
                system_prompt="ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿ä»é•¿æ–‡æ¡£ä¸­æå–æ ¸å¿ƒä¿¡æ¯å’Œå¯å¼•ç”¨ç´ æã€‚è¯·ç®€æ´ã€ç²¾å‡†åœ°è¾“å‡ºã€‚",
                user_message=summary_prompt,
                temperature=0.3,
                max_tokens=800
            )
            
            # è§£ææ‘˜è¦ç»“æœ
            material['ai_summary'] = summary_result
            material['is_summarized'] = True
            
            # æå–å…³é”®è¦ç‚¹ï¼ˆç®€å•è§£æï¼‰
            key_points = []
            if 'å…³é”®è¦ç‚¹' in summary_result:
                import re
                points = re.findall(r'\d+[.ã€](.+?)(?=\d+[.ã€]|å¯å¼•ç”¨|ä¸é€‰é¢˜|$)', summary_result, re.DOTALL)
                key_points = [p.strip()[:100] for p in points if p.strip()][:3]
            material['key_points'] = key_points
            
        except Exception as e:
            print(f"[WARN] ç´ ææ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            # å›é€€ï¼šä½¿ç”¨ç®€å•æˆªæ–­
            material['ai_summary'] = None
            material['summary'] = content[:300] + '...'
            material['key_points'] = []
        
        return material
    
    def _extract_topic_from_brief(self, brief_analysis: str) -> str:
        """
        ä»éœ€æ±‚åˆ†æä¸­æå–ä¸»é¢˜å…³é”®è¯ç”¨äºæœç´¢
        
        Args:
            brief_analysis: Step 1 ç”Ÿæˆçš„éœ€æ±‚åˆ†æ
            
        Returns:
            é€‚åˆæœç´¢çš„ä¸»é¢˜å…³é”®è¯å­—ç¬¦ä¸²
        """
        import re
        
        # å°è¯•ä»ç»“æ„åŒ–åˆ†æä¸­æå–ä¸»é¢˜
        topic_match = re.search(r'ä¸»é¢˜[ï¼š:]\s*(.+?)(?:\n|$)', brief_analysis)
        if topic_match:
            return topic_match.group(1).strip()
        
        # å°è¯•æå–å…³é”®è¯
        keywords_match = re.search(r'å…³é”®è¯[ï¼š:]\s*(.+?)(?:\n|$)', brief_analysis)
        if keywords_match:
            return keywords_match.group(1).strip()
        
        # å›é€€ï¼šæå–å‰å‡ ä¸ªå…³é”®è¯
        keywords = self._extract_keywords(brief_analysis)
        if keywords:
            return ' '.join(keywords[:5])
        
        # æœ€åå›é€€ï¼šè¿”å›å‰50ä¸ªå­—ç¬¦
        return brief_analysis[:50].replace('\n', ' ')
    
    async def execute_step_6(self) -> Dict[str, Any]:
        """
        Step 6: æŒ‚èµ·ç­‰å¾…ï¼ˆæ•°æ®ç¡®è®¤å¡ç‚¹ï¼‰
        """
        think_aloud = "â¸ï¸ æŒ‚èµ·ç­‰å¾…ç”¨æˆ·ç¡®è®¤æ‰€æœ‰å¿…éœ€ç´ æå·²å°±ç»ª..."
        
        result = """## æ•°æ®ç¡®è®¤æ¸…å•

è¯·ç¡®è®¤ä»¥ä¸‹å†…å®¹å·²å‡†å¤‡å¥½ï¼š
- [ ] çœŸå®æ¡ˆä¾‹å’Œç»å†
- [ ] ä¸ªäººè§‚ç‚¹å’Œæ€åº¦
- [ ] å¿…è¦çš„æ•°æ®æ”¯æŒ
- [ ] å…¶ä»–å…³é”®ä¿¡æ¯

âš ï¸ é‡è¦æé†’ï¼š
- ç»ä¸ç¼–é€ è™šå‡ä¿¡æ¯
- å®å¯ç­‰å¾…ä¹Ÿä¸çå†™
- æ‰€æœ‰æ•°æ®å¿…é¡»æœ‰æ¥æº

ç¡®è®¤æ— è¯¯åï¼Œç‚¹å‡»"ç»§ç»­"è¿›å…¥åˆ›ä½œé˜¶æ®µã€‚
"""
        
        return {
            "output": result,
            "think_aloud": think_aloud,
            "is_checkpoint": True  # å¡ç‚¹ï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤
        }
    
    async def execute_step_7(
        self,
        selected_topic: str,
        style_guide: str,
        materials: str,
        channel_id: str,
        word_count: int = 1500,  # é»˜è®¤å­—æ•°é™åˆ¶
        style_profile: Dict[str, Any] = None,  # é£æ ¼ç”»åƒ
        selected_sample: Dict[str, Any] = None,  # v3.5: æ‰€é€‰çš„å•ä¸€æ ‡æ†æ ·æ–‡
        knowledge_summary: str = ""  # v3.6: Step 2 è°ƒç ”æ‘˜è¦ï¼Œç”¨äºæ³¨å…¥äº‹å®åœ°åŸº
    ) -> Dict[str, Any]:
        """
        Step 7: åˆç¨¿åˆ›ä½œï¼ˆv3.6 - å•ä¸€æ ‡æ†é©±åŠ¨ + è°ƒç ”äº‹å®åœ°åŸºï¼‰
        
        æ ¸å¿ƒå˜åŒ–ï¼š
        1. ä¼˜å…ˆä½¿ç”¨æ‰€é€‰æ ·æ–‡çš„ç‹¬ç«‹ style_profileï¼ˆå•ä¸€æ ‡æ†ï¼‰
        2. å¦‚æœç”¨æˆ·ä¿®æ”¹äº†åˆ›ä½œæŒ‡å—ï¼Œäººå·¥å¹²é¢„è¦†ç›–æ ·æ–‡é»˜è®¤ç‰¹å¾
        3. ä¸¥ç¦å‡­ç©ºç¼–é€ æ¡ˆä¾‹
        4. v3.6 æ–°å¢ï¼šæ³¨å…¥ Step 2 è°ƒç ”æ‘˜è¦ï¼Œç¡®ä¿ä¸“ä¸šè®ºæ®æœ‰æ®å¯ä¾
        
        ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š
        1. ç”¨æˆ·ç‰¹æ®Šè¦æ±‚ (custom_requirement)
        2. ç”¨æˆ·ä¿®æ”¹çš„åˆ›ä½œæŒ‡å— (writing_guidelines)
        3. æ‰€é€‰æ ·æ–‡çš„ 6 ç»´ç‰¹å¾çº¦æŸ (selected_sample.style_profile)
        4. é¢‘é“åŸºæœ¬äººæ ¼è®¾å®š
        """
        channel_config = self.load_channel_config(channel_id)
        
        # ================================================================
        # v3.5: ç¡®å®šä½¿ç”¨çš„é£æ ¼æ¥æº
        # ================================================================
        effective_style_profile = None
        style_source = "é»˜è®¤"
        
        # ä¼˜å…ˆçº§1: ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰çš„ style_profileï¼ˆå¦‚æœæœ‰ï¼‰
        if style_profile and style_profile.get('is_customized'):
            effective_style_profile = style_profile
            style_source = "ç”¨æˆ·è‡ªå®šä¹‰"
        # ä¼˜å…ˆçº§2: ä½¿ç”¨æ‰€é€‰æ ·æ–‡çš„ç‹¬ç«‹ style_profile
        elif selected_sample and selected_sample.get('style_profile'):
            effective_style_profile = selected_sample['style_profile']
            style_source = f"æ ·æ–‡ã€Š{selected_sample.get('title', 'æœªçŸ¥')}ã€‹"
        # ä¼˜å…ˆçº§3: ä½¿ç”¨ä¼ å…¥çš„ style_profileï¼ˆå¯èƒ½æ˜¯é¢‘é“æ•´ä½“ç”»åƒï¼‰
        elif style_profile:
            effective_style_profile = style_profile
            style_source = "é¢‘é“é£æ ¼ç”»åƒ"
        
        # ================================================================
        # æå–é£æ ¼å…³é”®ä¿¡æ¯
        # ================================================================
        style_instructions = ""
        structural_logic = []
        writing_guidelines = []
        
        # è·å–ç»“æ„é€»è¾‘å’Œåˆ›ä½œæŒ‡å—
        if effective_style_profile:
            structural_logic = effective_style_profile.get('structural_logic', [])
            writing_guidelines = effective_style_profile.get('writing_guidelines', [])
            
            # ç›´æ¥ä½¿ç”¨æ ·æ–‡çš„ç‰¹å¾ï¼ˆä¸å†åŒºåˆ† stable_features å’Œ dimensionsï¼‰
            dims = effective_style_profile
        else:
            dims = {}
        
        if dims or structural_logic or writing_guidelines:
            style_instructions = """
## ğŸ¨ ã€å¼ºåˆ¶ã€‘é£æ ¼ DNA å¯¹é½ï¼ˆå¿…é¡» 100% éµå®ˆï¼‰

"""
            # ============================================================
            # 1. ç»“æ„é€»è¾‘ï¼ˆå¿…é¡»æŒ‰æ­¤é¡ºåºç»„ç»‡æ®µè½ï¼‰
            # ============================================================
            if structural_logic:
                style_instructions += "### ğŸ“ æ®µè½ç»“æ„ï¼ˆå¿…é¡»æŒ‰æ­¤é¡ºåºï¼‰\n"
                for i, step in enumerate(structural_logic[:6], 1):
                    style_instructions += f"  {i}. {step}\n"
                style_instructions += "\n**è¦æ±‚**ï¼šæ–‡ç« å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°ç»“æ„å®‰æ’æ®µè½ï¼Œä¸å¾—é—æ¼æˆ–ä¹±åºã€‚\n\n"
            
            # ============================================================
            # 2. å…­ç»´é£æ ¼ç‰¹å¾
            # ============================================================
            if dims:
                style_instructions += "### ğŸ­ å…­ç»´é£æ ¼ç‰¹å¾\n\n"
                
                # å¼€å¤´æ–¹å¼
                if dims.get('opening_style'):
                    opening = dims['opening_style']
                    style_instructions += f"**ã€å¼€å¤´ã€‘** ç±»å‹ï¼š{opening.get('type', 'æ•…äº‹å¼•å…¥')}\n"
                    if opening.get('description'):
                        style_instructions += f"  - è¦æ±‚ï¼š{opening['description']}\n"
                    if opening.get('examples'):
                        style_instructions += f"  - å‚è€ƒï¼šã€Œ{opening['examples'][0][:50]}...ã€\n"
                    style_instructions += "\n"
                
                # å¥å¼ç‰¹å¾
                if dims.get('sentence_pattern'):
                    sp = dims['sentence_pattern']
                    short_ratio = sp.get('short_ratio', 0.6)
                    style_instructions += f"**ã€å¥å¼ã€‘** çŸ­å¥å æ¯”ï¼š{short_ratio * 100:.0f}%\n"
                    if sp.get('favorite_punctuation'):
                        style_instructions += f"  - å¸¸ç”¨æ ‡ç‚¹ï¼š{', '.join(sp['favorite_punctuation'][:5])}\n"
                    if sp.get('description'):
                        style_instructions += f"  - ç‰¹å¾ï¼š{sp['description']}\n"
                    style_instructions += "\n"
                
                # æ®µè½èŠ‚å¥
                if dims.get('paragraph_rhythm'):
                    pr = dims['paragraph_rhythm']
                    style_instructions += f"**ã€æ®µè½ã€‘** èŠ‚å¥å˜åŒ–ï¼š{pr.get('variation', 'medium')}\n"
                    if pr.get('description'):
                        style_instructions += f"  - ç‰¹å¾ï¼š{pr['description']}\n"
                    style_instructions += "\n"
                
                # è¯­æ°”ç‰¹ç‚¹
                if dims.get('tone'):
                    tone = dims['tone']
                    formality = tone.get('formality', 0.3)
                    style_instructions += f"**ã€è¯­æ°”ã€‘** ç±»å‹ï¼š{tone.get('type', 'æ¸©æ¶¦äº²åˆ‡')}\n"
                    style_instructions += f"  - æ­£å¼åº¦ï¼š{formality * 100:.0f}%ï¼ˆ{'å£è¯­åŒ–' if formality < 0.4 else 'åŠæ­£å¼' if formality < 0.7 else 'æ­£å¼'}ï¼‰\n"
                    if tone.get('description'):
                        style_instructions += f"  - è¦æ±‚ï¼š{tone['description']}\n"
                    style_instructions += "\n"
                
                # ç»“å°¾é£æ ¼
                if dims.get('ending_style'):
                    ending = dims['ending_style']
                    style_instructions += f"**ã€ç»“å°¾ã€‘** ç±»å‹ï¼š{ending.get('type', 'å¼•å¯¼æ€è€ƒ')}\n"
                    if ending.get('description'):
                        style_instructions += f"  - è¦æ±‚ï¼š{ending['description']}\n"
                    style_instructions += "\n"
                
                # å¸¸ç”¨è¡¨è¾¾
                if dims.get('expressions'):
                    expr = dims['expressions']
                    if expr.get('high_freq_words'):
                        style_instructions += f"**ã€æ¨èç”¨è¯ã€‘** {', '.join(expr['high_freq_words'][:8])}\n"
                    if expr.get('avoid_words'):
                        style_instructions += f"**ã€ç¦æ­¢ç”¨è¯ã€‘** {', '.join(expr['avoid_words'][:8])}ï¼ˆä¸€æ—¦å‡ºç°å³ä¸ºä¸åˆæ ¼ï¼‰\n"
                    style_instructions += "\n"
            
            # ============================================================
            # 3. åˆ›ä½œæŒ‡å—ï¼ˆæ¯æ¡éƒ½æ˜¯ç¡¬æ€§è¦æ±‚ï¼‰
            # ============================================================
            if writing_guidelines:
                style_instructions += "### ğŸ“‹ åˆ›ä½œæŒ‡å—ï¼ˆæ¯æ¡éƒ½æ˜¯ç¡¬æ€§è§„åˆ™ï¼‰\n"
                for i, guideline in enumerate(writing_guidelines[:10], 1):
                    style_instructions += f"  {i}. âœ… {guideline}\n"
                style_instructions += "\n**å®¡æ ¡æ ‡å‡†**ï¼šä¸Šè¿°æ¯æ¡æŒ‡å—éƒ½å°†åœ¨ Step 8 å®¡æ ¡ä¸­é€ä¸€æ£€æŸ¥ï¼Œä¸ç¬¦åˆçš„å°†è¢«é€€å›ä¿®æ”¹ã€‚\n"
            
            # ============================================================
            # 4. æœ¬ç¯‡ç‰¹æ®Šè¦æ±‚ï¼ˆç”¨æˆ·è‡ªå®šä¹‰ï¼Œæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            # ============================================================
            custom_requirement = style_profile.get('custom_requirement') if style_profile else None
            if custom_requirement:
                style_instructions += f"\n### â­ æœ¬ç¯‡ç‰¹æ®Šè¦æ±‚ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰\n"
                style_instructions += f"ç”¨æˆ·æ˜ç¡®è¦æ±‚ï¼š{custom_requirement}\n"
                style_instructions += "**å¿…é¡»ä¸¥æ ¼æ‰§è¡Œä¸Šè¿°ç‰¹æ®Šè¦æ±‚ï¼Œä¸å¾—å¿½ç•¥ï¼**\n"
        
        # ================================================================
        # v3.7: æ„å»º System Promptï¼ˆç˜¦èº«ç‰ˆ - èšç„¦æ‰‹æ„Ÿï¼‰
        # ç§»é™¤ç¦ä»¤ç±»è§„åˆ™ï¼ˆå±è”½è¯ã€ç¦ç”¨ä¹¦ç›®ã€ä¸¥æ ¼ç¦æ­¢ï¼‰ï¼Œäº¤ç”± Step 8 å®¡æ ¡å¤„ç†
        # ä¼˜å…ˆçº§ï¼šè°ƒç ”èƒŒæ™¯ > æ ·æ–‡é£æ ¼ > ç”¨æˆ·ç‰¹æ®Šè¦æ±‚ > é¢‘é“è°ƒæ€§
        # ================================================================
        system_prompt = f"""{channel_config['system_prompt']['role']}

## ğŸ¯ æœ¬æ¬¡åˆ›ä½œçš„æ ¸å¿ƒç›®æ ‡
åœ¨è¿™ä¸€æ­¥ï¼Œè¯·**å…¨ç¥è´¯æ³¨äºæ–‡å­—çš„æµåŠ¨æ„Ÿå’Œå¯¹æ ·æ–‡é£æ ¼çš„ç²¾å‡†å¤åˆ»**ã€‚
æ— éœ€æ‹…å¿ƒè¿ç¦è¯æˆ–ç”¨è¯­è§„èŒƒï¼Œç¨åä¼šæœ‰ä¸“é—¨çš„å®¡æ ¡ç¯èŠ‚å¤„ç†è¿™äº›ç»†èŠ‚ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ï¼šå†™å‡ºæœ‰æ¸©åº¦ã€æœ‰èŠ‚å¥ã€æœ‰çœŸå®æ„Ÿçš„åˆç¨¿ã€‚

## ğŸ“Š å­—æ•°è¦æ±‚
- ç›®æ ‡å­—æ•°ï¼š{word_count}å­—
- å…è®¸èŒƒå›´ï¼š{int(word_count * 0.9)}å­— ~ {int(word_count * 1.1)}å­—ï¼ˆÂ±10%åå·®ï¼‰

{style_instructions}

## ğŸ“ é¢‘é“åŸºç¡€è°ƒæ€§
{chr(10).join(['- ' + style for style in channel_config['system_prompt']['writing_style']])}

## âœ… å¿…é¡»éµå®ˆ
{chr(10).join(['- ' + rule for rule in channel_config['channel_specific_rules']['must_do']])}

## âš ï¸ çœŸå®ç´ æçº¦æŸ
- æ–‡ä¸­æ‰€æœ‰æ¡ˆä¾‹ã€æ•…äº‹ã€å¼•ç”¨å¿…é¡»æ¥è‡ªä¸‹æ–¹æä¾›çš„ã€å¯ç”¨ç´ æã€‘æˆ–ã€è°ƒç ”èƒŒæ™¯ã€‘
- ä¸¥ç¦å‡­ç©ºç¼–é€ ä»»ä½•æ¡ˆä¾‹æˆ–æ•°æ®
- å¦‚æœç´ æä¸å¤Ÿç”¨ï¼Œè¯·ç®€åŒ–å†…å®¹è€Œä¸æ˜¯æé€ 
"""
        
        # æ„å»º Think Aloud (v3.7 ä¸“æ³¨æ‰‹æ„Ÿæ¨¡å¼)
        is_customized = effective_style_profile.get('is_customized', False) if effective_style_profile else False
        custom_req = effective_style_profile.get('custom_requirement', '') if effective_style_profile else ''
        has_knowledge = bool(knowledge_summary and knowledge_summary.strip())
        
        think_aloud = f"âœï¸ å¼€å§‹åˆ›ä½œåˆç¨¿ (ä¸“æ³¨æ‰‹æ„Ÿæ¨¡å¼)...\n\n"
        think_aloud += f"ğŸ“ é¢‘é“ï¼š{channel_config['channel_name']}\n"
        think_aloud += f"ğŸ“ å­—æ•°è¦æ±‚ï¼š{word_count}å­—\n"
        think_aloud += f"ğŸ“ é£æ ¼æ¥æºï¼š{style_source}\n"
        
        # v3.7: æ˜¾ç¤ºä¼˜å…ˆçº§é¡ºåº
        think_aloud += "\nğŸ¯ æœ¬æ¬¡åˆ›ä½œä¼˜å…ˆçº§ï¼š\n"
        if has_knowledge:
            think_aloud += f"  1ï¸âƒ£ è°ƒç ”èƒŒæ™¯ï¼ˆ{len(knowledge_summary)}å­—æ‘˜è¦ï¼‰\n"
        else:
            think_aloud += "  1ï¸âƒ£ è°ƒç ”èƒŒæ™¯ï¼ˆæ— ï¼‰\n"
        think_aloud += f"  2ï¸âƒ£ æ ·æ–‡é£æ ¼ DNAï¼ˆ{style_source}ï¼‰\n"
        if custom_req:
            think_aloud += f"  3ï¸âƒ£ ç‰¹æ®Šè¦æ±‚ï¼š{custom_req[:30]}...\n"
        else:
            think_aloud += "  3ï¸âƒ£ ç‰¹æ®Šè¦æ±‚ï¼ˆæ— ï¼‰\n"
        think_aloud += "  4ï¸âƒ£ é¢‘é“åŸºç¡€è°ƒæ€§\n"
        
        think_aloud += "\nğŸ’¡ ä¸“æ³¨æ¨¡å¼ï¼šæœ¬æ­¥éª¤èšç„¦æ–‡å­—æµåŠ¨æ„Ÿä¸é£æ ¼å¤åˆ»ï¼Œè¿ç¦è¯æ£€æŸ¥å°†åœ¨ Step 8 æ‰§è¡Œ"
        
        # ================================================================
        # v3.6: æ„å»ºè°ƒç ”èƒŒæ™¯æ¿å—ï¼ˆä»…åœ¨æœ‰è°ƒç ”æ•°æ®æ—¶æ³¨å…¥ï¼‰
        # ================================================================
        knowledge_section = ""
        if has_knowledge:
            knowledge_section = f"""## è°ƒç ”èƒŒæ™¯ï¼ˆæ¥è‡ª Step 2 æ·±åº¦è°ƒç ”ï¼‰
{knowledge_summary}

> âš ï¸ è¯·ç»“åˆä¸Šè¿°è°ƒç ”èƒŒæ™¯è¿›è¡Œåˆ›ä½œï¼Œç¡®ä¿æ–‡ç« çš„ä¸“ä¸šè®ºæ®ä¸è°ƒç ”ç»“è®ºä¿æŒä¸€è‡´ã€‚

"""
        
        user_message = f"""è¯·åˆ›ä½œæ–‡ç« åˆç¨¿ã€‚

{knowledge_section}## é€‰é¢˜
{selected_topic}

## é£æ ¼æŒ‡å—
{style_guide}

## å¯ç”¨ç´ æï¼ˆåªèƒ½ä½¿ç”¨è¿™äº›ï¼Œç¦æ­¢ç¼–é€ ï¼‰
{materials}

## âš ï¸ åˆ›ä½œè¦æ±‚
1. æ–‡ç« æ€»å­—æ•°ï¼š{int(word_count * 0.9)} ~ {int(word_count * 1.1)} å­—ï¼ˆå…è®¸Â±10%åå·®ï¼‰
2. ä¸¥æ ¼æ¨¡ä»¿é£æ ¼ç”»åƒä¸­çš„å¼€å¤´æ–¹å¼ã€å¥å¼èŠ‚å¥ã€è¯­æ°”ç‰¹ç‚¹
3. çœŸå®ç´ æè‡ªç„¶èå…¥ï¼Œç¦æ­¢å‡­ç©ºç¼–é€ æ¡ˆä¾‹
{f"4. ç»“åˆã€è°ƒç ”èƒŒæ™¯ã€‘ä¸­çš„ä¸“ä¸šè®ºæ®ï¼Œç¡®ä¿å†…å®¹æœ‰äº‹å®æ”¯æ’‘" if has_knowledge else ""}

è¯·å¼€å§‹åˆ›ä½œï¼Œç›´æ¥è¾“å‡ºæ–‡ç« å†…å®¹ã€‚
"""
        
        # æ ¹æ®å­—æ•°åŠ¨æ€è°ƒæ•´ max_tokensï¼ˆä¸­æ–‡çº¦ 1.5 å­—ç¬¦/tokenï¼‰
        estimated_tokens = min(int(word_count * 1.5), 4000)
        
        result = await ai_service.generate_content(
            system_prompt=system_prompt,
            user_message=user_message,
            temperature=0.7,
            max_tokens=estimated_tokens
        )
        
        return {
            "output": result,
            "think_aloud": think_aloud
        }
    
    async def execute_step_8(
        self, 
        draft: str, 
        channel_id: str, 
        word_count: int = 1500,
        style_profile: Dict[str, Any] = None  # é£æ ¼ç”»åƒ
    ) -> Dict[str, Any]:
        """
        Step 8: çºªå¾‹å®¡æ ¡æœºåˆ¶ï¼ˆv3.7 - æ¥ç®¡æ‰€æœ‰ç¦ä»¤æ£€æŸ¥ï¼‰
        
        èŒè´£ï¼š
        1. ç¬¬ä¸€éï¼šå» AI è…” - å…¨å±€å±è”½è¯ + é¢‘é“ä¸¥æ ¼ç¦æ­¢é¡¹
        2. ç¬¬äºŒéï¼šé»‘åå•æ ¡éªŒ - ç¦ç”¨ä¹¦ç›®æ£€æŸ¥
        3. ç¬¬ä¸‰éï¼šé£æ ¼ DNA å¯¹é½æ£€æŸ¥
        4. ç¬¬å››éï¼šç»†èŠ‚æ‰“ç£¨ + å­—æ•°æ§åˆ¶
        
        åé¦ˆæœºåˆ¶ï¼šå‘ç°è¿ç¦å†…å®¹æ‰§è¡Œå±€éƒ¨é‡å†™ï¼Œè€Œéè®© Step 7 å…¨å±€é‡æ¥
        """
        channel_config = self.load_channel_config(channel_id)
        blocked_words_config = self.load_blocked_words()
        writing_constraints = self.load_writing_constraints()
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½ç¦ç”¨ä¹¦ç›®
        banned_books_config = writing_constraints.get('banned_books', {})
        banned_books_list = ''.join(banned_books_config.get('list', []))
        banned_books_hint = banned_books_config.get('replacement_hint', 'è¯·é€‰æ‹©æ›´å°ä¼—ä½†åŒæ ·ä¼˜è´¨çš„ä½œå“')
        
        # è®¡ç®—å½“å‰è‰ç¨¿å­—æ•°ï¼ˆå…è®¸ Â±10% åå·®ï¼‰
        current_word_count = len(draft)
        max_allowed = int(word_count * 1.1)  # ä¸Šé™ï¼šç›®æ ‡å­—æ•°çš„110%
        min_allowed = int(word_count * 0.9)  # ä¸‹é™ï¼šç›®æ ‡å­—æ•°çš„90%
        is_over_limit = current_word_count > max_allowed
        
        # ================================================================
        # æ„å»ºå±è”½è¯æ›¿æ¢è¡¨ï¼ˆè§„åˆ™ä¼˜å…ˆ + AI å…œåº•ï¼‰
        # ================================================================
        blocked_phrases_with_replacement = []
        for category in blocked_words_config['categories'].values():
            category_name = category.get('name', 'æœªåˆ†ç±»')
            for pattern in category['patterns']:
                blocked_phrases_with_replacement.append(
                    f"| {pattern['phrase']} | {pattern['replacement']} | {pattern['reason']} |"
                )
        
        # é¢‘é“ä¸¥æ ¼ç¦æ­¢é¡¹
        channel_must_not_do = channel_config['channel_specific_rules'].get('must_not_do', [])
        
        # ================================================================
        # æ„å»ºé£æ ¼ DNA å¯¹é½æ£€æŸ¥æ¸…å•
        # ================================================================
        style_checklist = ""
        if style_profile:
            dims = style_profile.get('stable_features') or style_profile.get('dimensions', {})
            structural_logic = style_profile.get('structural_logic', [])
            writing_guidelines = style_profile.get('writing_guidelines', [])
            
            style_checklist = """
## ğŸ¯ é£æ ¼ DNA å¯¹é½æ£€æŸ¥ï¼ˆå¿…é¡»é€é¡¹æ‰“åˆ†ï¼‰

è¯·å¯¹ç…§ä»¥ä¸‹æ ‡å‡†æ£€æŸ¥æ–‡ç« ï¼Œæ¯é¡¹æ‰“åˆ† âœ“ï¼ˆç¬¦åˆï¼‰æˆ– âœ—ï¼ˆä¸ç¬¦ï¼‰ï¼š

### ç»“æ„æ£€æŸ¥
"""
            # ç»“æ„é€»è¾‘æ£€æŸ¥
            if structural_logic:
                style_checklist += "æ–‡ç« æ®µè½æ˜¯å¦æŒ‰ä»¥ä¸‹é¡ºåºç»„ç»‡ï¼š\n"
                for i, step in enumerate(structural_logic[:6], 1):
                    style_checklist += f"  {i}. [ ] {step}\n"
            
            # å…­ç»´ç‰¹å¾æ£€æŸ¥
            style_checklist += "\n### å…­ç»´ç‰¹å¾æ£€æŸ¥\n"
            
            if dims.get('opening_style'):
                opening = dims['opening_style']
                style_checklist += f"1. [å¼€å¤´] æ˜¯å¦é‡‡ç”¨ã€Œ{opening.get('type', 'æ•…äº‹å¼•å…¥')}ã€æ–¹å¼ï¼Ÿ [ ]\n"
            
            if dims.get('sentence_pattern'):
                sp = dims['sentence_pattern']
                short_ratio = sp.get('short_ratio', 0.6)
                style_checklist += f"2. [å¥å¼] çŸ­å¥å æ¯”æ˜¯å¦ â‰¥ {short_ratio * 100:.0f}%ï¼Ÿ [ ]\n"
                if sp.get('favorite_punctuation'):
                    style_checklist += f"   - æ˜¯å¦ä½¿ç”¨äº†è¿™äº›æ ‡ç‚¹ï¼š{', '.join(sp['favorite_punctuation'][:3])}ï¼Ÿ [ ]\n"
            
            if dims.get('tone'):
                tone = dims['tone']
                style_checklist += f"3. [è¯­æ°”] æ˜¯å¦ä¸ºã€Œ{tone.get('type', 'æ¸©æ¶¦äº²åˆ‡')}ã€é£æ ¼ï¼Ÿ [ ]\n"
                formality = tone.get('formality', 0.3)
                style_checklist += f"   - æ­£å¼åº¦æ˜¯å¦çº¦ {formality * 100:.0f}%ï¼Ÿ [ ]\n"
            
            if dims.get('ending_style'):
                ending = dims['ending_style']
                style_checklist += f"4. [ç»“å°¾] æ˜¯å¦é‡‡ç”¨ã€Œ{ending.get('type', 'å¼•å¯¼æ€è€ƒ')}ã€æ–¹å¼ï¼Ÿ [ ]\n"
            
            if dims.get('expressions'):
                expr = dims['expressions']
                if expr.get('avoid_words'):
                    style_checklist += f"5. [ç¦è¯] æ˜¯å¦ä½¿ç”¨äº†ç¦æ­¢ç”¨è¯ {', '.join(expr['avoid_words'][:5])}ï¼Ÿ [ ] ï¼ˆå¿…é¡»ä¸º âœ—ï¼‰\n"
            
            # åˆ›ä½œæŒ‡å—æ£€æŸ¥
            if writing_guidelines:
                style_checklist += "\n### åˆ›ä½œæŒ‡å—æ£€æŸ¥\n"
                style_checklist += "è¯·é€æ¡æ£€æŸ¥æ˜¯å¦éµå®ˆï¼š\n"
                for i, guideline in enumerate(writing_guidelines[:10], 1):
                    style_checklist += f"  {i}. [ ] {guideline}\n"
            
            style_checklist += """
### å¯¹é½è¯„åˆ†
- æ€»åˆ† = ç¬¦åˆé¡¹æ•° / æ€»é¡¹æ•° Ã— 100%
- åˆæ ¼çº¿ï¼šâ‰¥ 80%
- è‹¥ä¸åˆæ ¼ï¼Œå¿…é¡»ä¿®æ”¹åé‡æ–°è¾“å‡º
"""
        
        # æ ¹æ®æ˜¯å¦è¶…å­—æ•°è°ƒæ•´å®¡æ ¡è¦æ±‚ï¼ˆå…è®¸ Â±10% åå·®ï¼‰
        word_count_instruction = ""
        if is_over_limit:
            word_count_instruction = f"""
## âš ï¸ å­—æ•°è¶…é™è­¦å‘Š
- å½“å‰å­—æ•°ï¼š{current_word_count}å­—
- ç›®æ ‡å­—æ•°ï¼š{word_count}å­—ï¼ˆå…è®¸èŒƒå›´ï¼š{min_allowed}~{max_allowed}å­—ï¼‰
- è¶…å‡ºä¸Šé™ï¼š{current_word_count - max_allowed}å­—
- ã€å¿…é¡»æ‰§è¡Œã€‘åœ¨å®¡æ ¡è¿‡ç¨‹ä¸­ç²¾ç®€å†…å®¹ï¼Œåˆ é™¤å†—ä½™è¡¨è¾¾ï¼Œç¡®ä¿æœ€ç»ˆç‰ˆæœ¬ä¸è¶…è¿‡{max_allowed}å­—
"""
        else:
            word_count_instruction = f"""
## å­—æ•°æ£€æŸ¥
- å½“å‰å­—æ•°ï¼š{current_word_count}å­—
- ç›®æ ‡å­—æ•°ï¼š{word_count}å­—ï¼ˆå…è®¸èŒƒå›´ï¼š{min_allowed}~{max_allowed}å­—ï¼‰
- å­—æ•°ç¬¦åˆè¦æ±‚ âœ“
"""
        
        # ================================================================
        # v3.8: ç¦ç”¨ä¹¦ç›®ä»é…ç½®æ–‡ä»¶åŠ è½½ï¼ˆwriting_constraints.jsonï¼‰
        # ================================================================
        
        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å†…å®¹å®¡æ ¡ä¸“å®¶ï¼Œè´Ÿè´£çºªå¾‹æŠŠå…³å’Œæœ€ç»ˆæ¶¦è‰²ã€‚
è¯·å¯¹æ–‡ç« è¿›è¡Œ**å››éä¸“é¡¹å®¡æ ¡**ï¼Œå‘ç°é—®é¢˜ç›´æ¥**å±€éƒ¨é‡å†™**ä¿®å¤ï¼Œæ— éœ€è¿”å›ä¸Šä¸€æ­¥ã€‚

{word_count_instruction}

---

## ğŸ”´ ç¬¬ä¸€éï¼šå» AI è…”ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰

### å…¨å±€å±è”½è¯æ›¿æ¢è¡¨
è¯·é€ä¸€æ£€æŸ¥æ–‡ç« ä¸­æ˜¯å¦åŒ…å«ä»¥ä¸‹è¯æ±‡ï¼Œå¦‚æœ‰åˆ™**å¿…é¡»æ›¿æ¢**ï¼š

| ç¦ç”¨çŸ­è¯­ | æ›¿æ¢ä¸º | åŸå›  |
|---------|-------|------|
{chr(10).join(blocked_phrases_with_replacement[:25])}

### é¢‘é“å±è”½è¯
ä»¥ä¸‹è¡¨è¾¾ç¦æ­¢å‡ºç°ï¼š{', '.join(channel_config['blocked_phrases'])}

### é¢‘é“ä¸¥æ ¼ç¦æ­¢é¡¹
{chr(10).join(['- âŒ ' + rule for rule in channel_must_not_do])}

**æ›¿æ¢åŸåˆ™**ï¼š
1. **ä¼˜å…ˆä½¿ç”¨ä¸Šè¡¨ä¸­çš„ã€Œæ›¿æ¢ä¸ºã€å»ºè®®**
2. å¦‚æœæ›¿æ¢å»ºè®®ä¸é€‚åˆå½“å‰è¯­å¢ƒï¼Œå¯è‡ªè¡Œè°ƒæ•´ï¼Œä½†éœ€ä¿æŒå£è¯­åŒ–ã€æœ‰æ¸©åº¦
3. å‘ç°è¿ç¦è¯åï¼Œç›´æ¥åœ¨åŸæ–‡ä½ç½®è¿›è¡Œå±€éƒ¨é‡å†™ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯

---

## ğŸŸ¡ ç¬¬äºŒéï¼šé»‘åå•æ ¡éªŒ

### ç¦ç”¨ä¹¦ç›®ï¼ˆé¿å… AI å‘³ï¼‰
æ£€æŸ¥æ–‡ç« ä¸­æ˜¯å¦å¼•ç”¨äº†ä»¥ä¸‹è¢«è¿‡åº¦å¼•ç”¨çš„ä¹¦ç±ï¼š
{banned_books_list}

**å¤„ç†æ–¹å¼**ï¼š
- {banned_books_hint}
- æˆ–è€…åˆ é™¤è¯¥ä¹¦åå¼•ç”¨ï¼Œæ”¹ç”¨æ³›åŒ–æè¿°

---

## ğŸŸ¢ ç¬¬ä¸‰éï¼šé£æ ¼ DNA å¯¹é½

{style_checklist}

---

## ğŸ”µ ç¬¬å››éï¼šç»†èŠ‚æ‰“ç£¨ + å­—æ•°æ§åˆ¶

- å¥å­é•¿åº¦ï¼šæ‹†åˆ†è¶…è¿‡ 40 å­—çš„é•¿å¥
- æ®µè½é•¿åº¦ï¼šæ¯æ®µä¸è¶…è¿‡ 200 å­—
- æ ‡ç‚¹ç¬¦å·ï¼šæ£€æŸ¥ä½¿ç”¨æ˜¯å¦è‡ªç„¶
- è‡ªç„¶è¯­è°ƒï¼šè¯»èµ·æ¥åƒäººåœ¨è¯´è¯
- ã€é‡è¦ã€‘ç¡®ä¿æ€»å­—æ•°åœ¨ {min_allowed}~{max_allowed} å­—èŒƒå›´å†…

---

## ğŸ“‹ è¾“å‡ºæ ¼å¼

### å®¡æ ¡æŠ¥å‘Š

#### ç¬¬ä¸€éï¼šå» AI è…”
| åŸæ–‡ | æ›¿æ¢ä¸º | ä½ç½® |
|-----|-------|------|
| xxx | xxx | ç¬¬xæ®µ |

#### ç¬¬äºŒéï¼šé»‘åå•æ ¡éªŒ
- [ ] æœªå‘ç°ç¦ç”¨ä¹¦ç›® / å·²æ›¿æ¢ï¼šxxx â†’ xxx

#### ç¬¬ä¸‰éï¼šé£æ ¼ DNA å¯¹é½
| æ£€æŸ¥é¡¹ | ç»“æœ | è¯´æ˜ |
|-------|------|-----|
| å¼€å¤´æ–¹å¼ | âœ“/âœ— | ... |
| ... | ... | ... |

**å¯¹é½åˆ†æ•°ï¼šxx%**

#### ç¬¬å››éï¼šç»†èŠ‚æ‰“ç£¨
- å­—æ•°ï¼šå½“å‰ xxx å­—ï¼ˆ{"éœ€è¦ç²¾ç®€" if is_over_limit else "ç¬¦åˆè¦æ±‚"}ï¼‰
- é•¿å¥æ‹†åˆ†ï¼šx å¤„
- æ®µè½è°ƒæ•´ï¼šx å¤„

---

### ä¿®æ”¹åç‰ˆæœ¬
ï¼ˆè¾“å‡ºå®Œæ•´çš„ä¿®æ”¹åæ–‡ç« ï¼Œç¡®ä¿æ‰€æœ‰å®¡æ ¡é—®é¢˜å·²ä¿®å¤ï¼‰
"""
        
        think_aloud = f"ğŸ” å¼€å§‹çºªå¾‹å®¡æ ¡ï¼ˆv3.7ï¼‰...\n\n"
        think_aloud += f"ğŸ“Š å½“å‰å­—æ•°ï¼š{current_word_count}å­—ï¼ˆç›®æ ‡ï¼š{word_count}å­—ï¼Œå…è®¸èŒƒå›´ï¼š{min_allowed}~{max_allowed}å­—ï¼‰\n\n"
        think_aloud += "ğŸ“‹ å››éä¸“é¡¹å®¡æ ¡æµç¨‹ï¼š\n"
        think_aloud += "  ğŸ”´ ç¬¬ä¸€éï¼šå» AI è…”ï¼ˆå±è”½è¯æ›¿æ¢ + é¢‘é“ç¦æ­¢é¡¹ï¼‰\n"
        think_aloud += "  ğŸŸ¡ ç¬¬äºŒéï¼šé»‘åå•æ ¡éªŒï¼ˆç¦ç”¨ä¹¦ç›®æ£€æŸ¥ï¼‰\n"
        think_aloud += "  ğŸŸ¢ ç¬¬ä¸‰éï¼šé£æ ¼ DNA å¯¹é½æ£€æŸ¥\n"
        think_aloud += "  ğŸ”µ ç¬¬å››éï¼šç»†èŠ‚æ‰“ç£¨ + å­—æ•°æ§åˆ¶\n\n"
        think_aloud += "ğŸ’¡ åé¦ˆæœºåˆ¶ï¼šå‘ç°è¿ç¦å†…å®¹å°†æ‰§è¡Œå±€éƒ¨é‡å†™ï¼Œæ— éœ€è¿”å› Step 7"
        
        # åŠ¨æ€è°ƒæ•´ max_tokens
        estimated_tokens = min(int(word_count * 2.5), 8000)  # é¢„ç•™å®¡æ ¡æŠ¥å‘Šç©ºé—´
        
        result = await ai_service.generate_content(
            system_prompt=system_prompt,
            user_message=f"è¯·å¯¹ä»¥ä¸‹æ–‡ç« è¿›è¡Œå››éå®¡æ ¡ï¼ˆå­—æ•°å…è®¸èŒƒå›´ï¼š{min_allowed}~{max_allowed}å­—ï¼Œå¯¹é½åˆ†æ•° â‰¥ 80%ï¼‰ï¼š\n\n{draft}",
            temperature=0.3,
            max_tokens=estimated_tokens
        )
        
        return {
            "output": result,
            "think_aloud": think_aloud
        }
    
    async def execute_step_9(self, final_article: str) -> Dict[str, Any]:
        """
        Step 9: æ–‡ç« é…å›¾
        """
        system_prompt = """ä½ æ˜¯é…å›¾æ–¹æ¡ˆä¸“å®¶ã€‚æ ¹æ®æ–‡ç« å†…å®¹ï¼Œæä¾›é…å›¾å»ºè®®ã€‚

è¾“å‡ºæ ¼å¼ï¼š
## é…å›¾æ–¹æ¡ˆ

### å›¾1ï¼šæ ‡é¢˜/ä½ç½®
- æè¿°ï¼šxxx
- é£æ ¼ï¼šæ’ç”»/ç…§ç‰‡/å›¾è¡¨
- AIç»˜å›¾æç¤ºè¯ï¼šxxx

### å›¾2ï¼šæ ‡é¢˜/ä½ç½®
...

## Markdownä»£ç 
```markdown
![å›¾1æè¿°](å›¾ç‰‡è·¯å¾„)

æ–‡ç« å†…å®¹...

![å›¾2æè¿°](å›¾ç‰‡è·¯å¾„)
```

æ³¨æ„ï¼š
- é…å›¾è¦ä¸å†…å®¹ç›¸å…³
- 5-8å¼ ä¸ºå®œ
- æä¾›æ¸…æ™°çš„AIç»˜å›¾æç¤ºè¯
"""
        
        think_aloud = "ğŸ–¼ï¸ æ­£åœ¨ç”Ÿæˆé…å›¾æ–¹æ¡ˆ..."
        
        result = await ai_service.generate_content(
            system_prompt=system_prompt,
            user_message=f"ä¸ºä»¥ä¸‹æ–‡ç« æä¾›é…å›¾æ–¹æ¡ˆï¼š\n\n{final_article[:2000]}...",
            temperature=0.5
        )
        
        return {
            "output": result,
            "think_aloud": think_aloud
        }

# å…¨å±€å·¥ä½œæµå¼•æ“å®ä¾‹
workflow_engine = WorkflowEngine()

